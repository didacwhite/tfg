\documentclass{beamer}
\title
{Resolución numérica del problema no lineal de mínimos cuadrados.
Aplicaciones a las estimación de parámetros de modelos matemáticos.}

\author
{Dídac Blanco Morros}


\date
{14 de febrero de 2023}

\titlegraphic{\vspace{0.7cm}\hspace{1.5cm}\includegraphics[height=1.6cm]{logomath.pdf}}
%\logo{\includegraphics[height=1cm]{overleaf-logo}}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Índice.}
    \tableofcontents[currentsection]
  \end{frame}
}

%%% PREAMBULO
\usepackage[spanish]{babel}	

\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{tikz}
\newlist{steps}{enumerate}{1}
\setlist[steps, 1]{label = Paso \arabic*., leftmargin=1cm, topsep=0pt}

\usepackage{biblatex}
\addbibresource{tfg.bib}

\usepackage{listings}
\usepackage{matlab-prettifier}

\definecolor{azulUSC}{RGB}{13,38,120}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\sucesionxk}{\left\{x_k\right\}}
\newcommand{\sucesion}[1]{\left\{#1\right\}}
\newcommand{\citeline}{
    \begin{tikzpicture}
        \draw[thick, azulUSC] (0,0) -- (10,0);
    \end{tikzpicture}
}

\def\code#1{\texttt{#1}}

%%% FIN DO PREAMBULO
\begin{document}
\frame{\titlepage}

\begin{frame}
\frametitle{Índice.} % Table of contents title
\tableofcontents
\end{frame}

\section{Fundamentos de la optimización sin restricciones}
\begin{frame}
    \frametitle{Conceptos previos}
    \begin{block}{}
        Un \alert{problema de optimización sin restricciones} tiene la forma
        \begin{equation*}
            \min_x f(x)
        \end{equation*}
        \end{block}
    \begin{itemize}
        \item $x\in \mathbb{R}^n$ y $f:\mathbb{R}^n \to \mathbb{R}$ es continuamente diferenciable.
        \item A $f$ se le llama función objetivo.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conceptos previos}
    \begin{itemize}[label=\textbullet]
    \item Norma euclídea: $
            \norm{x}_2 = \left( \sum_{i=1}^n \vert x_i \vert^2 \right)^{1/2}.
        $
    \item Punto estacionario de $f$: $\nabla f(x) = 0$.
    \item Mínimo $x^*$: existe $\delta > 0$ tal que $f(x^*) \leq f(x)$.
    \item Mínimo estricto $x^*$: existe $\delta > 0$ tal que $f(x^*) < f(x)$ con $x \neq x^*$.
    \begin{itemize}[label=-]
        \item Local: para todo $x \in \mathbb{R}^n$ que satisface $\Vert x - x^* \Vert < \delta$. 
        \item Global: para todo $x \in \mathbb{R}^n$
    \end{itemize}
    \item Producto escalar de $x$ e $y$ en $\mathbb{R}^n$: $\langle x,y \rangle = \sum_{i=1}^n x_i y_i$.
    \item Dirección descendente de $f$ en $x$: $d \in \mathbb{R}^n$ tal que $\langle \nabla f(x), d \rangle < 0$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Condiciones de optimalidad}
    Consideremos el problema inicial
    \begin{equation*}
        \min_x f(x)
    \end{equation*}\pause
    \vspace{-0.5cm}
    \begin{block}{Teorema: Condición Necesaria de Primer Orden}
        Sea $f:D\subset \mathbb{R}^n \rightarrow \mathbb{R}$ continuamente diferenciable en un
        conjunto abierto $D$. Si $x^*$ es un mínimo local, entonces
        $\nabla f(x^*) = 0$.
    \end{block}\pause

    \begin{block}
        {Teorema: Condición Necesaria de Segundo Orden}
        Sea $f:D\subset \mathbb{R}^n \rightarrow \mathbb{R}$ dos veces continuamente diferenciable
        en un conjunto abierto $D$. Si $x^*$ es un mínimo local, entonces
        $\nabla f(x^*) = 0$ y $\nabla^2 f(x^*)$ es definida positiva.
    \end{block}\pause
    
    \begin{block}{Teorema: Condición Suficiente de Segundo Orden}
        Sea $f:D\subset \mathbb{R}^n \rightarrow \mathbb{R}$ dos veces continuamente diferenciable
        en un conjunto abierto $D$. Si $\nabla f(x^*) = 0$ y $\nabla^2 f(x^*)$ es definida positiva,
        entonces $x^* \in D$ es un mínimo local.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Convexidad}
    \begin{block}
        {Conjunto convexo D} Sea $S\subset \mathbb{R}^n$ y sean $x_1, x_2 \in S$ cualesquiera. Si
        $\alpha x_1 + (1 - \alpha)x_2 \in S$ para todo $\alpha \in [0,1]$
    \end{block}
    \begin{block}
        {Función convexa $f$ en $S$} 
        Sean $S \subset \mathbb{R}^n$ un conjunto convexo no vacío y $f$ una función de $S$ en $\mathbb{R}$.
        Si para cualquiera $x_1, x_2 \in S$ y
        $\alpha \in (0,1)$, se cumple que
        \begin{equation*}
            f(\alpha x_1 + (1-\alpha)x_2) \leq \alpha f(x_1) + (1-\alpha)f(x_2),
        \end{equation*}
    \end{block}
\end{frame}
\begin{frame}
    \frametitle{Convexidad}
    \begin{block}{Teorema}
        Sea $S \subset \mathbb{R}^n$ un conjunto convexo no vacío y
        $f:S \subset \mathbb{R}^n \rightarrow \mathbb{R}$ una función convexa. Si $x^*$ es mínimo
        local, entonces también es mínimo global.
    \end{block}
    \begin{block}{Teorema}
        Sea $f: \mathbb{R}^n \rightarrow \mathbb{R}$ convexa y diferenciable, entonces $x^*$ es un
        mínimo global si y solo si $\nabla f(x^*) = 0$.
    \end{block}
\end{frame}
\begin{frame}
    \frametitle{Algoritmos. ¿Qué son?}
    \begin{itemize}[label=\textbullet]
        \item Conjunto de órdenes con normas y condiciones. \pause
        \item Estructura de bucles iterativos. \pause
        \item Eficiencia computacional. \pause
        \item Mínimos locales y globales.
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Algoritmos. Esquema.}
    \begin{steps}
		\item Paso inicial. Se definen las condiciones iniciales, suelen ser el primer
		punto de la sucesión $\sucesionxk$, valores para definir las condiciones de
		parada como el número máximo de iteraciones o el error aceptable, y otros parámetros.
        \pause
		\item Test de parada. Se comprueba si se cumple alguna condición de parada.
        \pause
		\item Proceso principal. Se realizan los cálculos necesarios para avanzar de
		$x_k$ a $x_{k+1}$.
        \pause
		\item Actualización y bucle. Se actualiza el valor de $x_k$ a $x_{k+1}$, así como otros parámetros necesarios. Se repite el proceso desde el \textit{paso 2.}
	\end{steps}
\end{frame}

\begin{frame}
    \frametitle{Búsqueda de línea.}
    \begin{itemize}[label=\textbullet]
        \item Se toma el punto inicial $x_k$ (para cada iteración).
        \item Elige una dirección $d_k$ con la misma dimensión que $x_k$.
        \begin{itemize}[label=-]
            \item La mayoría eligen una dirección de descenso, esto es, $d_k^T \nabla f_k < 0$.
            \item Suele tener la forma $d_k = -B_k^{-1}\nabla f_k$.
        \end{itemize}
        \item Se elige una longitud de paso $\alpha_k\in \mathbb{R}$.
        \item Se obtiene $x_{k+1} = x_k + \alpha_k d_k$.
        \begin{itemize}[label=-]
            \item El objetivo es que $f(x_{k+1}) < f(x_k)$.
            \item Si se se soluciona el problema $\min_{\alpha_k} f(x_k + \alpha_k d_k)$, búsqueda de línea exacta.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Región de confianza.}
    \begin{itemize}[label=\textbullet]
        \item Se toma una pequeña región donde es más fácil predecir como se comporta la función.
        \item Primero se fija una distancia máxima $\Delta_k$ para definir una región
        \begin{equation*}
            \Omega_k = \left\{x : \Vert x-x_k \Vert \leq \Delta_k \right\}.
        \end{equation*}
        \item Para predecir el comportamiento en esta región, se utilizan aproximaciones como la siguiente:
        \begin{equation*}
            m_k(p) := q^{(k)}(p) = f(x_k) + g^T_kp + \frac{1}{2}p^TG_kp.
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Región de confianza.}
    \begin{block}{Subproblema de optimización con restricciones}
        \begin{equation*}
            \begin{aligned}
                \min_{p} \quad & m_k(p) = f(x_k) + g^T_kp + \frac{1}{2}p^TB_kp \\
                \text{s.a.} \quad & \Vert p \Vert \leq \Delta_k,
            \end{aligned}
        \end{equation*}
    \end{block} \pause 
    \vspace{1cm}
    \citeline\\
    \vspace{0.4cm}
    \fullcite{Nocedal2006-kh} \\
    \vspace{0.4cm}
    \fullcite{Sun2006-au}
\end{frame}

\begin{frame}
    \frametitle{Región de confianza.}
    \begin{block}{Teorema: Caracterización de la solución del subproblema}
        El vector $p^*$ es una solución global si y solo si $p^*$ es factible y existe un escalar $\lambda^* \geq 0$ tal que:
        \begin{itemize}[label=\textbullet]
            \item $(B+\lambda^* I)p^*=-g,$
	        \item $\lambda^* (\Delta - \norm{p^*}) = 0,$
	        \item $(B+\lambda^* I)$ es semidefinida positiva.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Región de confianza.}
    \begin{block}{Ratio de aceptación}
        Para tener en cuenta la semejanza entre la aproximación y la función real, se define el ratio de aceptación:
        \begin{equation*}
            \rho_k = \frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m(p_k)},
        \end{equation*}
        el cual será más grande cuanto más parecida sea la aproximación a la función real. Se tiene en cuenta para la elección de $\Delta_k$.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Región de confianza. Esquema.}
    \begin{steps}
		\item Dados $x_0, \bar{\Delta}, \Delta_0 \in (0, \bar{\Delta}), \epsilon \geq
			0, 0<\eta_1\leq\eta_2<1$ y $0<\gamma_1<1<\gamma_2, k:=0$.
		\item Si $\norm{g_k} \leq \epsilon$ terminar.
		\item Aproximar $p_k$ resolviendo según la caracterización.
		\item Calcular $f(x_k+p_k)$ y $\rho_k$. Definir
			\begin{equation*}
				x_{k+1} = \begin{cases}
					x_k + p_k, & \text{si } \rho_k \geq \eta_1, \\
					x_k, & \text{en otro caso.}
				\end{cases}
			\end{equation*}
		\item Si $\rho_k < \eta_1$ entonces $\Delta_{k+1} \in (0,\gamma_1 \Delta_k]$. \\
			Si $\rho_k \in [\eta_1, \eta_2)$ entonces $\Delta_{k+1} \in [\gamma_1 \Delta_k,\Delta_k]$. \\
			Si $\rho_k \geq \eta_2$ y $\norm{p_k}=\Delta_k$ entonces
			$\Delta_{k+1} \in [\Delta_k, \min \{\gamma_2 \Delta_k,\bar{\Delta}\}]$.
		\item Calcular $B_{k+1}$, actualizar $m^{(k)}$ y $k:=k+1$. Ir al Paso 2.
	\end{steps}
\end{frame}

\section{Mínimos Cuadrados}

\begin{frame}
    \frametitle{El problema.}
    \begin{equation*}
        \min_{x\in \mathbb{R}^{n}}f(x) = \frac{1}{2}\sum_{i=1}^{m}r_i^2(x), \quad m\geq n,
    \end{equation*}
    \pause
\begin{columns}
    \begin{column}{0.4\textwidth}
        \begin{itemize}[label=\textbullet]
            \item $x$ es el parámetro a estimar de dimensión $n$. \pause
            \item $\phi_x(t)$ es la función a ajustar. \pause 
            \item $r_i(x) = \phi_x(t_i) - y_i$, para $i=1,\dots,m$ son los residuos. \pause
        \end{itemize}
    \end{column}
    \begin{column}{0.6\textwidth}  %%<--- here
        \begin{figure}[h]
            \centering
            \includegraphics[width=1\textwidth]{imgs/examplelsqlin.png}
            \caption{Ejemplo de ajuste de una recta a una muestra de puntos}
        \end{figure}
    \end{column}
\end{columns}
\end{frame}

\begin{frame}
    \frametitle{Ejemplo de ajuste de una recta a una muestra de puntos.}
    \begin{figure}[h]
        \centering
        \includegraphics[width=1\textwidth]{imgs/examplelsqlin.png}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Ejemplo de ajuste de una exponencial a una muestra de puntos.}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/exp.png}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{El método de Gauss-Newton.}
    \begin{itemize}[label=\textbullet]
        \item Se trata de una linealización del problema de mínimos cuadrados. \pause
        \item Para linealizar, se desprecia el término cuadrático de $r_i(x)$ y se obtiene el caso lineal:
        \begin{equation*}
            f(x) = \frac{1}{2} \Vert Jx-y \Vert^2.
        \end{equation*}
        \begin{equation*}
            \nabla f(x) = J^T(Jx-y), \qquad \nabla^2 f(x) =  J^TJ.
        \end{equation*}
        \pause
        \item Se resuelve el sistema lineal $J^TJx = J^Ty$. \pause
        \item Convergencia local y cuadrática.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{El método de Gauss-Newton.}
    \begin{steps}
        \item Dados $x_0$ y $\epsilon > 0$, $k:=0$.
        \item Si $\norm{g_k} \leq \epsilon$, terminar.
        \item Obtener el paso $p_k$ resolviendo
            \begin{equation}
                J(x_k)^TJ(x_k)p_k = -J(x_k)^Tr(x_k).
            \end{equation}
        \item Definimos $x_{k+1} = x_k + p_k$ y actualizamos $k=k+1$. Ir a Paso 2.
    \end{steps}

\end{frame}


\section{El método de Levenberg-Marquardt}

\begin{frame}
    \frametitle{El método de Levenberg-Marquardt.}
    \begin{itemize}[label=\textbullet]
        \item Se mantiene la idea de la linealización de Gauss-Newton. \pause
        \item Se cambia de enfoque a región de confianza. \pause
        \begin{equation*} \hspace{-1cm}
            \begin{aligned}
                \min_{p} &\quad m_k(p)=\frac{1}{2}\norm{r_k}^2 + p^TJ_k^Tr_k + \frac{1}{2}p^TJ_k^TJ_kp, \\
             \text{s.a.} &\quad \norm{p} \leq \Delta_k.
            \end{aligned}
        \end{equation*} \pause
    \end{itemize}
    \begin{block}{Lema: Caracterización de la solución.}
        El vector $p$ es solución del subproblema si y solo si $p$ es factible y existe un $\lambda \geq 0$ tal que
        \begin{align*}
	        (J^TJ+\lambda I)p&=-J^Tr, \label{eq:lmsol}\\
	        \lambda (\Delta-\norm{p})&=0.
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Propiedades del método de Levenberg-Marquardt.}
    \begin{itemize}[label=\textbullet]
        \item Convergencia global bajo ciertas condiciones.
        \item El ratio de convergencia es variable (ver Teorema 3.7).
    \end{itemize}
    \vspace{2cm}
    \citeline\\
    \vspace{0.3cm}
    \fullcite{Sun2006-au}
\end{frame}

\begin{frame}
    \frametitle{Implementación del método de Levenberg-Marquardt.}
    \begin{block}{Versión de Moré}
        Disponemos de la implementación de este método propuesta por Jorge J. Moré,
        con un estudio detallado del mismo en su artículo\footfullcite{More1978-at}.
    \end{block} \pause
    \begin{block}{Veremos a continuación en detalle:}
        \begin{itemize}[label=\textbullet]
            \item Actualización del radio $\Delta_k$. \pause
            \item El parámetro Levenberg-Marquardt. \pause
            \item Matriz de escalado $D_k$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementación del método de Levenberg-Marquardt.}
    \begin{block}{Actualización del radio $\Delta_k$}
        En este caso particular el ratio viene dado por
        \begin{equation*}
	        \rho = \frac{\norm{r(x_k)}^2-\norm{r(x_k+p_k)}^2}{\norm{r(x_k)}^2-\norm{r(x_k)+J(x_k)p_k}^2},
        \end{equation*}
        y obtenemos la siguiente expresión explícita:
        \begin{equation*}
            \rho = \frac{1-\left[\frac{\norm{r(x_k+p_k)}}{\norm{r(x_k)}}\right]^2}
                        {\left[\frac{\norm{J_kp}}{\norm{r(x_k)}}\right]^2
                        + 2\left[\frac{\lambda^{1/2}\norm{D_kp}}{\norm{r(x_k)}}\right]^2}.
        \end{equation*}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementación del método de Levenberg-Marquardt.}
    \begin{block}{El parámetro Levenberg-Marquardt}
        En esta implementación, se acepta $\alpha>0$ como parámetro de Levenberg-Marquardt Si
        \begin{equation*}
            |\phi(\alpha)| \leq \sigma\Delta,
        \end{equation*}
        siendo
        \begin{equation*}
            \phi(\alpha) = \norm{D(J^TJ+\alpha D^TD)^{-1}J^Tr}-\Delta,
        \end{equation*}
        y $\sigma \in (0,1)$ es un parámetro que controla la aceptación de $\alpha$. 
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementación del método de Levenberg-Marquardt.}
    \begin{steps}
        \item Definir valores iniciales $\alpha_0$, $l_0$ y $u_0$. Definir coeficiente de parada $\sigma$.
        \item Si $\alpha_k \notin (l_k, u_k)$, definir $\alpha_k = \max \{0.001u_k, (l_ku_k)^{1/2}\}$.	
        \item Calcular $\phi(\alpha_k)$ y $\phi'(\alpha_k)$. Si $|\phi(\alpha)|<\sigma\Delta$, terminar.
        \item Actualizar $l_k$ y $u_k$:
            \begin{align*}
                l_{k+1} &= \max \{l_k, \alpha_k-\frac{\phi(\alpha_k)}{\phi'(\alpha_k)}\}, \\
                u_{k+1} &= \begin{cases}
                    \alpha_k, & \text{si } \phi(\alpha_k) < 0, \\
                    u_k, & \text{en otro caso}.
                \end{cases}
            \end{align*}
        \item Actualizar $\alpha_{k+1}$ según
        \begin{equation*}
            \alpha_{k+1} = \alpha_k - \left[\frac{\phi(\alpha_k)+\Delta}{\Delta}\right]
                            \left[\frac{\phi(\alpha_k)}{\phi'(\alpha_k)}\right].
        \end{equation*}
        Volver al Paso 2.
    \end{steps}
\end{frame}

\begin{frame}
    \frametitle{Implementación del método de Levenberg-Marquardt.}
    \begin{block}{Escalado}
        Para reducir posibles problemas de escalado, se utiliza una matriz diagonal $D_k$ definido de la siguiente forma:
        \begin{equation*}
            D_k = diag(d_1^{(k)}, \ldots, d_n^{(k)}),
        \end{equation*}
        con
        \begin{equation*}
            d_i^{(k)}=\norm{\partial_ir(x_k)}, k\geq 0.
        \end{equation*}
        En cada paso, se actualiza cada $d_i^{(k)}$ si este aumenta, es decir,
        \begin{equation*}
            d_i^{(k)} = \max \{d_i^{(k-1)}, \norm{\partial_ir(x_k)}\}, k\geq 1.
        \end{equation*}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Esquema del algoritmo. I.}
    \begin{steps}
        \item Definir valores iniciales $x_0$, $\Delta_0$ y $\epsilon$ y determinar $J_k$, $D_k$, y $f(x_0)$ a partir de la función $f$ determinada.
        \item Si $\norm{J_k(D_k)^{-1}f(x_k)} < \epsilon$, terminar.
        \item Sea $\sigma \in (0,1)$. Si $\norm{D_kJ_k^{-}r_k} \leq (1+\sigma) \Delta_k$,
        asignar $\lambda_k = 0$ y $p_k = -J_k^{-}r_k$. En otro caso, encontar $\lambda_k$
        tal que si
        \begin{equation*}
            \left[
                \begin{array}{c}
                    J_k \\
                    \lambda_k^{\frac{1}{2}}D_k
                \end{array}
            \right]p_k
            \cong
            -\left[
                \begin{array}{c}
                    r_k \\
                    0
                \end{array}
            \right],
        \end{equation*}
        entonces
        \begin{equation*}
            (1-\sigma)\Delta_k \leq \norm{D_kp_k} \leq (1+\sigma)\Delta_k.
        \end{equation*}
        \item Determinar la relación $\rho_k$ entre reducción real y la esperada.
    \end{steps}
\end{frame}

\begin{frame}
    \frametitle{Esquema del algoritmo. II.}
    \begin{steps}[start=5]
    \item Si $\rho_k \leq 0.0001$, asignar $x_{k+1} = x_k$ y $J_{k+1} = J_k$. \\
    Si $\rho_k > 0.0001$, asignar $x_{k+1} = x_k + p_k$ y calcular $J_{k+1} = J(x_{k+1})$.
    \item Si $\rho_k \leq \frac{1}{4}$, asignar $\Delta_{k+1} \in \left[
        \frac{1}{10}\Delta_k, \frac{1}{4}\Delta_k \right]$. \\
        Si $\rho_k \in \left[ \frac{1}{4}, \frac{3}{4} \right]$ y $\lambda_k = 0$,
        o si $\rho_k \geq \frac{3}{4}$,
        asignar $\Delta_{k+1} = 2\norm{D_kp_k}$.
    \item Actualizar $D_k$ y volver al Paso 2.
\end{steps}
\end{frame}

\section{Implementación en Matlab\textsuperscript{®}}


\begin{frame}[fragile]
    \frametitle{Inicialización.}
    \begin{steps}
        \item Definir valores iniciales $x_0$, $\Delta_0$ y $\epsilon$ y determinar $J_k$, $D_k$, y $f(x_0)$ a partir de la función $f$ determinada.
    \end{steps} \pause
    \vspace{0.5cm}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
function [xk, iter] = myLevMar(fun, x0)
% setup
[n,m,xk,Delta,funk,lambdak, maxit, maxit2, eps]= myLevMarSetup(fun,x0);

% calculo inicial de Jk y Dk
dkm1=zeros(length(xk));
[Jk, Dk, dkm1] = JDk(fun,xk,dkm1);
\end{lstlisting}
        
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inicialización.}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
function [n,m,xk,Delta,funk,lambdak, maxit, maxit2, eps] = myLevMarSetup(fun,x0)
% editable
Delta = 3;
lambdak = 1; 
maxit = 50;
maxit2 = 10; % miteraciones maximas en hebden.m
eps = 0.001;

% no editable
n = length(x0);
m = length(fun(x0));
xk = x0;
funk = fun(xk)';
\end{lstlisting}
        
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inicialización.}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
function [Jk,Dk,v] = JDk(fun,xk,dkm1)

Jk = jacob(fun,xk);

% Compute diagonal of Dk
di = @(i) max(dkm1(i),norm(Jk(:,i)));

v = arrayfun(di, 1:length(xk));
Dk = diag(v);
\end{lstlisting}
        
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inicialización.}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
function J = jacob(fun,xk)
h = 1e-5;
n = length(xk);
m = length(fun(xk));
J = zeros(m,n);
for j = 1:n
    hj = zeros(1,n);
    hj(j) = h;
    J(:,j) = (fun(xk+hj)-fun(xk))./h;
end
\end{lstlisting}
        
\end{frame}


\begin{frame}[fragile]
    \frametitle{Test de parada.}
\begin{steps}[start=2]
    \item Si $\norm{J_k(D_k)^{-1}f(x_k)} < \epsilon$, terminar.\pause
\end{steps}
\vspace{0.5cm}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
try
    test = norm((Jk*inv(Dk))'*funk);
catch
    test = norm((Jk*inv(Dk))'*funk');
end
iter=0;
while test > eps && iter < maxit
\end{lstlisting}
        
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bucle.}
    \begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
% solucion problema lineal
% descomposicion QR
[Q,R,Pi] = qr(Jk); % calcula Jk*Pi = Q*R
% ajuste de R
T = R(1:n,1:n);
S = R(1:n,n+1:end);

% matriz $J^{-}$
Jm = Jmfun(Pi,T,Q,n,m);
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bucle.}
\begin{steps}[start=3]
    \item Sea $\sigma \in (0,1)$. Si $\norm{D_kJ_k^{-}r_k} \leq (1+\sigma) \Delta_k$,
        asignar $\lambda_k = 0$ y $p_k = -J_k^{-}r_k$. En otro caso, encontar $\lambda_k$
        tal que si
        \begin{equation*}
            \left[
                \begin{array}{c}
                    J_k \\
                    \lambda_k^{\frac{1}{2}}D_k
                \end{array}
            \right]p_k
            \cong
            -\left[
                \begin{array}{c}
                    r_k \\
                    0
                \end{array}
            \right],
        \end{equation*}
        entonces $(1-\sigma)\Delta_k \leq \norm{D_kp_k} \leq (1+\sigma)\Delta_k$.
\end{steps}\pause
\vspace{0.5cm}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
% parametro levenberg-marquardt
[pk, lambdak] = pkfun(Delta, Jk, Dk, funk, Jm, Q, Pi, n, m, lambdak, maxit2);
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Bucle.}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
function [pk, lambdak] = pkfun(Delta, Jk, Dk, funk, Jm, Q, Pi, n, m, lambdak, maxit)

phi = @(a) norm(Dk*inv(Jk'*Jk+a.*(Dk'*Dk))*Jk'*funk) - Delta;
if phi(0) == 0
    lambdak = 0;
    pk = -Jm*funk;
    pk = pk';
else
    lambdak = hebden(phi, Delta, Jk, Dk, funk, Q, Pi, n, m, lambdak, maxit);
    pk = pk2(funk, Q, Pi, Jk, Dk, lambdak, n, m);
end
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bucle.}
\begin{steps}[start=4]
    \item Determinar la relación $\rho_k$ entre reducción real y la esperada.
\end{steps}\pause
\vspace{0.3cm}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
rho = rhofun(funk,pk,lambdak,Dk,Jk,fun, xk);
\end{lstlisting}\pause
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
function rho = rhofun(funk,pk,lambdak,Dk,Jk,fun,xk)
funplus = fun(xk+pk);

rhonum=1-(norm(funplus)/norm(funk))^2;
rhoden=(norm(Jk*pk')/norm(funk))^2+2*(sqrt(lambdak)*norm(Dk*pk')/norm(funk))^2;
rho = rhonum/rhoden;
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bucle.}
\begin{steps}[start=5]
    \item Si $\rho_k \leq 0.0001$, asignar $x_{k+1} = x_k$ y $J_{k+1} = J_k$. \\
        Si $\rho_k > 0.0001$, asignar $x_{k+1} = x_k + p_k$ y calcular $J_{k+1} = J(x_{k+1})$.
\end{steps}\pause
\vspace{0.1cm}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
if rho <= 0.0001
    % xk+1 = xk y jk+1=jk
else
    xk = xk+pk;
    % calculo de Jk, Dk con el nuevo xk
    funk = fun(xk)';
    [Jk, Dk, dkm1] = JDk(fun,xk,dkm1);
    iter=iter+1;
    test = norm((Jk*inv(Dk))'*fun(xk)');
end
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bucle.}
\begin{steps}[start=6]
    \item Si $\rho_k \leq \frac{1}{4}$, asignar $\Delta_{k+1} \in \left[
            \frac{1}{10}\Delta_k, \frac{1}{4}\Delta_k \right]$. \\
            Si $\rho_k \in \left[ \frac{1}{4}, \frac{3}{4} \right]$ y $\lambda_k = 0$,
            o si $\rho_k \geq \frac{3}{4}$,
            asignar $\Delta_{k+1} = 2\norm{D_kp_k}$.
\end{steps}\pause
\vspace{0.5cm}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
% actualizacion de Delta
if rho <= 0.25
    Delta = Delta/2;
elseif rho >= 0.75
    Delta = 2*norm(Dk*pk');
elseif lambdak == 0
    Delta = 2*norm(Dk*pk');
end
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bucle.}
\begin{steps}[start=7]
    \item Actualizar $D_k$ y volver al Paso 2.
\end{steps}\pause
\vspace{0.5cm}
\begin{lstlisting}[frame=single, numbers=left, style=Matlab-editor]
    end
xk = real(xk);
\end{lstlisting}
\end{frame}
%%%%%%%%%%%%

\begin{frame}
    \frametitle{Fin.}
    \vspace{1cm}
    \begin{center}
        Resolución numérica del problema no lineal de mínimos cuadrados.
Aplicaciones a las estimación de parámetros de modelos matemáticos. \\

        \vspace{1cm}
        Dídac Blanco Morros.
        \vspace{2cm}
    \end{center}
    \begin{center}
        \hspace{1cm}
        \includegraphics[width=0.5\textwidth]{logomath.pdf}
    \end{center}
\end{frame}
%%%%%%%%%%%%
\end{document}

