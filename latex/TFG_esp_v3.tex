%%%%%%%%%%     PREÁMBULO      %%%%%%%%%%%%%%%

%%%%%%%%%   Non modificar este preámbulo, salvo que se sepa ben o que se está a facer   %%%%%%%%%

%%%%%%  Traballando nun Mac, se usas TeXShop, en  Preferencias -> Código -> codificación de  caracteres (UTF-8)  %%%%

\documentclass[11pt,a4paper]{book}

% Formato do documento
\usepackage[papersize={210mm,297mm},lmargin=2.5cm,rmargin=2.5cm,top=3.5cm,bottom=3.5cm]{geometry}   % Marxes
\renewcommand{\baselinestretch}{1.25}                                                               % Fixamos o interliñado
\setlength{\parskip}{8pt}                                                                           % Separación entre parágrafos


\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{latexsym,pdfsync,xcolor,graphicx}
\usepackage[nottoc]{tocbibind}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}					
\usepackage[spanish]{babel}			
\usepackage{tikz}
\usepackage{appendix}
\usepackage{hyperref}








% Cabeceira
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{\textbf{\thechapter. #1}}{}} % Formato para o capítulo: N. Nome
\renewcommand{\sectionmark}[1]{\markright{\textbf{\thesection. #1}}}  % Formato para a sección: N.M. Nome

\fancyhead[LO]{\rightmark}           % Nas páxinas impares, parte esquerda do encabezado, aparecerá o nome do capítulo
\fancyhead[RE]{\leftmark}            % Nas páxinas pares, parte dereita do encabezado, aparecerá o nombre da sección
\fancyhead[RO,LE]{\textbf{\thepage}} % Números de páxina nas esquinas dos encabezados




%%%   \pagestyle{plain}   Cando os títulos dos capítulos son moi longos, a opción por defecto das cabeceiras pode ser inapropiada;
%%%  nestes e noutros casos semellantes, pódese usar a opción "plain", que suprime as cabeceiras, deixando soamente a numeración das páxinas.


%%%%%%%      NON ESCRIBIR AQUÍ  (COMANDO PORTADA) %%%%%%%%%%%

\newcommand{\portada}[3]{
\thispagestyle{empty}

\definecolor{azulUSC}{RGB}{13,38,120}

\includegraphics[width=13cm]{logomath.pdf}

\vspace*{2cm}

\centerline{{\Large\bf  Traballo Fin de Grao}}

\vspace{2.6cm}

{\center{\Huge\bfseries\color{azulUSC} #1}\par}		

\vspace{1cm}

{\Large

\centerline{#2}		

\vspace{6.4cm}

\centerline{\sf #3}        

\vspace{1.25cm}

\centerline{\sf UNIVERSIDADE DE SANTIAGO DE COMPOSTELA}
}

\enlargethispage{1cm}
\clearpage
}

%%%%%%%      NON ESCRIBIR AQUÍ  (COMANDO PRIMEIRA PÁXINA) %%%%%%%%%%%

\newcommand{\primeira}[3]{
\thispagestyle{empty}
\enlargethispage{1cm}
\begin{center}\Large

{\sf GRAO DE MATEM\'ATICAS}

\bigskip
{\bfseries Traballo Fin de Grao}
\vspace{4cm}

{\bfseries\Huge #1} 		
\vspace{1.2cm}

{#2}		\par		     
\end{center}

\vspace{6.5cm}

\begin{center}
\Large
{#3}	\par			    

\vspace{1cm}
{\sf UNIVERSIDADE DE SANTIAGO DE COMPOSTELA}
\end{center}
\clearpage
}

%%%%%%%    Teoremas   %%%%%%%%%

\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{corollary}[theorem]{Corolario}
\newtheorem{algorithm}[theorem]{Algoritmo}
\newtheorem{axiom}[theorem]{Axioma}
\newtheorem{case}[theorem]{Caso}
\newtheorem{conclusion}[theorem]{Conclusión}
\newtheorem{condition}[theorem]{Condición}
\newtheorem{conjetura}[theorem]{Conjetura}				
\newtheorem{criterion}[theorem]{Criterio}
\newtheorem{ejercicio}[theorem]{Ejercicio}	
			
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definición}
\newtheorem{ejemplo}[theorem]{Ejemplo}				
\newtheorem{agradecimientos}[theorem]{Agradecimientos}	

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Observación}
\newtheorem{notation}[theorem]{Notación}
\newtheorem{problem}[theorem]{Problema}
\newtheorem{solution}[theorem]{Solución}

\addto\captionsspanish{
	\renewcommand{\contentsname}{\'Indice} 
	\renewcommand\appendixname{Anexo}
	\renewcommand\appendixpagename{Anexos}
}
%%%%%%%%%%%%%%%%%%   Fin do Preámbulo   %%%%%%%%%%%%%%%%




\begin{document}
\frontmatter

%%%%%%%%%%%      PORTADA      %%%%%%%%%%%%

\portada
{Resolución numérica del problema no lineal de mínimos cuadrados.\vspace{5pt}
Aplicaciones a la estimación de parámetros de modelos matemáticos.}		%  escribir aquí o  Título
{Dídac Blanco Morros}		%  escribir aquí o/a  Autor/a
{Curso Acad\'emico}		  	%  escribir aquí a Data de presentación, na forma:  mes, ano

%%%%%%%%     PRIMEIRA PÁXINA    %%%%%%%%%%%%%%%%%%

\mbox{}
\thispagestyle{empty}
\clearpage

\setcounter{page}{1}

\primeira
{Resolución numérica del problema no lineal de mínimos cuadrados. 
Aplicaciones a la estimación de parámetros de modelos matemáticos.}		%  escribir aquí o  Título
{Dídac Blanco Morros}		%  escribir aquí o/a  Autor/a
{Febrero, 2022}		  	%  escribir aquí a Data de presentación, na forma:  mes, ano

\thispagestyle{empty}

%%%%%%%%%%%      PROPOSTA DE TRABALLO      %%%%%%%%%%%%%%%%

\vspace*{.5 cm}

\chapter*{Trabajo propuesto}

\vspace{1 cm}

\begingroup
\renewcommand*{\arraystretch}{2.2}
\begin{tabular}{|l|}
	\hline
	
	{\bf \'Area de Co\~necemento:  \/ }\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}
			\bf T\'{\i}tulo:   \/ \bf
			\vspace{.2cm}}
	\end{minipage}\\ \hline
	\bf Breve descrici\'on do contido\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}

		
		\vspace{.2cm}}
	\end{minipage}\\ \hline
	{\bf Recomendaci\'ons}\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}


		\vspace{.2cm}}
	\end{minipage}\\ \hline
	{\bf Outras observaci\'ons}\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}


			\vspace{.2cm}}
	\end{minipage}\\ \hline
\end{tabular}
\endgroup

\clearpage

\thispagestyle{empty}

\tableofcontents

\clearpage

\thispagestyle{empty}

\mbox{}

\clearpage

%%%%%%%%%     RESUMO---RESUMEN---ABSTRACT     %%%%%%%%%%%%%

\phantomsection
\addcontentsline{toc}{chapter}{Resumen}		
\chapter*{}

\section*{Resumen}

% escribir aquí resumo en español




\vspace{1.5cm}

\section*{Abstract}



% escribir aquí resumo en inglés



\clearpage

\thispagestyle{empty}

%%%%%%%%%    INTRODUCIÓN     %%%%%%%%%%%%%%%%%%
%%%%%%%%%    De non incluír unha INTRODUCIÓN, suprímese    %%%%%%%%%%

\phantomsection
\chapter*{Introducci\'on}\addcontentsline{toc}{chapter}{Introducci\'on}

\markboth{INTRODUCCI\'ON}{INTRODUCCI\'ON}

%%%%%%%%%%%     fin da INTRODUCIÓN     %%%%%%%%%%%%%

\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				S
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				 T
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				  A
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				   R
%%%%%%%%     Póñense os Capítulos    %%%%%%%%%%%%%%%				    T



\chapter{Fundamentos de la optimización sin restricciones}
% El problema de los mínimos cuadrados es un caso particular de optimización sin restricciones, y es por ello que comenzaremos introduciendo sus fundamentos. Ya que el problema de mínimos cuadrados es usado en multitud de campos para estimar parámetros, este es de los más utilizados dentro de los problemas de optimización sin restricciones.

Un problema de optimización sin restricciones tiene la forma 
\begin{equation}
	\min_{x}f\left(x\right)
	\label{eq:minf}
\end{equation}
donde $x\in\mathbb{R}^{n}$ y $f : \mathbb{R}^{n} \rightarrow \mathbb{R}$ es continuamente diferenciable, la llamamos \textbf{función objetivo}. La dificultad de un problema como este viene de no conocer el comportamiento global de $f$, normalmente solo disponemos de la evaluación de f en algunos puntos, y a lo mejor de algunas de sus derivadas. El trabajo de los algoritmos de optimización es identificar la solución sin usar demasiado tiempo ni almacenamiento computacional.

Notar que podemos usar la formulación (\ref{eq:minf}) para referirnos tanto a los problemas de minimización como de maximización, basta sustituir $f$ por $-f$. 

% W. Sun es más formal en este caso para definir ambos mínimos 
Tenemos dos tipos de solución.
Un punto $x^{*}$ se dice \textbf{mínimo global} si $f(x^{*})\leq f(x)$ para todo $x\in\mathbb{R}^{n}$. Como no se suele tener un conocimiento a gran escala de $f$ debido a su coste, la mayoría de algoritmos solo encuentran mínimos locales, lo cual es suficiente para muchos casos prácticos. Un punto $x^{*}$ se dice \textbf{mínimo local} si existe una vecinidad $\mathcal{V}$ de $x^{*}$ tal que $f(x^{*})\leq f(x)$ para todo $x\in\mathcal{V}$.

Aún así, los algoritmos para encontrar mínimos globales se suelen construír a partir de una secuencia de otros algoritmos de optimización local. También podemos aprovechar características fáciles de detectar en la función objetivo, como la convexidad, que nos asegura que un mínimo local será también global.

%% REPASO DE ALGORITMOS

%% \section{Generalidades de los algoritmos}

%% histórico??
% referencia 	NOCEDAL 2.2
Todo algoritmo de optimización sin restricciones comienza con un punto de partida, denotado normalmente como $x_{0}$. Aunque generalmente el usuario introduce una estimación razonable, el punto puede ser elegido por el algoritmo, tanto de forma sistemática como aleatoria. El algoritmo itera sobre $x_{0}$, creando una sucesión $\left\{x_k\right\}_{k=0}^n$ la cual termina cuando no pueda continuar o cuando ya se haya acercado razonablemente a la solución. Para decidir como se avanza de un $x_k$ al siguiente, los algoritmos utilizan información sobre $f(x_k)$ o incluso en los puntos anteriores $x_0,\, x_1,\, \ldots,\,x_{k-1}$ con el objetivo de que $f(x_{k+1})<f(x_{k})$. Hablaremos de las dos estrategias fundamentales que se utilizan para avanzar de $x_k$ a $x_{k+1}$, \textit{búsqueda de línea} y \textit{región de confianza}.

\section{Búsqueda de línea}

En este caso el algoritmo tiene dos tareas a partir de cada iteración, primero elige una \textit{dirección} $d_k$ y tomando el punto de partida busca en esa dirección el nuevo valor. Es decir, dado $x_k$
\begin{equation}
	x_{k+1} = x_k + \alpha_kd_k
\end{equation}
para un $d_k$ elegido previamente, y un \textit{paso} $\alpha_k$ obtenido solucionando otro problema de minimización más simple por ser unidimensional:
\begin{equation}
	\min_{\alpha_k>0}f\left(x_k+\alpha_kd_k\right).
	\label{min:alphak}
\end{equation}

Si se toma el $\alpha_k$ óptimo se le llama búsqueda de línea \textit{exacta} u \textit{óptima}. Para evitar el gran coste computacional que puede llegar a tomar, lo más común es tomar un $\alpha_k$ que aporte un descenso aceptable, en cuyo caso se le llama búsqueda de línea \textit{inexacta} o \textit{aproximada}. Desde el nuevo punto se busca otra dirección y paso para repetir el proceso.
Veamos brevemente cómo se eligen $d_k$ y $\alpha_k$.

La mayor parte de algoritmos de este tipo necesitan que $d_k$ sea una dirección descendente, esto es, $d_k^T \nabla f_k < 0$,
lo cual asegura que en esa dirección se podrá reducir el valor de $f$. Esta suele tener la forma
\begin{equation}
	d_k = -B_k^{-1} \nabla f_k
\end{equation}
con $B_k$ una aproximación de la matriz Hessiana $\nabla^2 f(x_k)$ simétrica y no singular. Según lo que acabamos de decir, necesitamos que $B_k$ sea definida positiva. En las tres corrientes principales se elige un $B_k$ distinto, en el \textit{método del descenso máximo} o \textit{descenso del gradiente}, se usa la matriz identidad $I$.
En el \textit{método de Newton} se usa la matriz exacta, mientras que en los \textit{métodos Quasi-Newton} la matriz Hessiana es aproximada para cada $x_k$.

En el caso de la elección de $\alpha_k$, el caso ideal sería encontrar el óptimo en \ref{min:alphak}, pero esto es en general demasiado costoso.
Debido a ese coste, se suelen utilizar búsquedas inexactas probando una serie de puntos hasta que alguno cumpla unas condiciones preestablecidas con las que se acepta el paso dado.
Estas condiciones son por ejemplo las condiciones \textit{Wolfe} o las condiciones \textit{Goldstein}.
Esta elección se hace en dos fases, primero un proceso elige un intervalo conteniendo los pasos deseables y una segunda fase donde se va reduciendo el intervalo por técnicas de interpolación o bisección. % el mejor?



\section{Región de confianza}

Esta estrategia enfoca el problema de otro modo, primero se fija una distancia máxima $\Delta_k$ para definir la región, que generalmente es de la forma
\begin{equation}
	\Omega_k = \left\{x : \Vert x-x_k \Vert \leq \Delta_k \right\}
\end{equation}
y luego ya se busca la dirección y paso.
A partir de la información conocida de $f$, para cada $x_k$ se modela una función $m_k$ que se comporte de manera similar a $f$ cerca de este punto.
Generalmente se utiliza el modelo cuadrático de la forma
\begin{equation}
	m_k := q^{(k)}(p) = f(x_k) + g^T_kp + \frac{1}{2}p^TG_kp,
\end{equation}
donde $g_k = \nabla f(x_k)$ y $G_k = \nabla^2f(x_k)$.
Este modelo cuadrático es el utilizado en los métodos de búsqueda de línea para determinar la dirección de búsqueda, mientras que en este caso lo usamos para tener una representación adecuada de la función objetivo y así elegir el mínimo dentro de esta región.
Este método nos evita el problema de que la Hessiana no sea definida positiva.
En cada iteración, una vez elegido $\Delta_k$ se resuelve el siguiente problema:
\begin{equation}
\begin{aligned}
	\min_{p} \quad & q^{(k)}(p) = f(x_k) + g^T_kp + \frac{1}{2}p^TB_kp \\
	\text{s.a.} \quad & \Vert p \Vert \leq \Delta_k. \\
\end{aligned}
\end{equation}
Notamos que en el modelo se escribe $B_k$ en lugar de $G_k$, pues no siempre se usa esta última.
Debido al coste computacional, como vimos en la elección de la dirección de búsqueda, a veces se prefiere aproximar de alguna manera más o menos eficiente, e incluso puede ser aceptable tomar la matriz $0$.

También se puede elegir qué norma define la región de confianza, cambiando así la forma de esta y ofreciendo distintos resultados, aunque generalmente se utiliza la bola definida por $\Vert p \Vert_2 \leq \Delta_k$.

La efectividad de cada iteración depende de la elección del radio $\Delta_k$, es por ello que puede que la primera elección de este no sea la definitiva. Es decir, se toma un radio a raíz de la información que se tenga, esta puede incluír la de pasos anteriores, y luego se decide si este radio nos da un resultado aceptable. Un radio demasiado pequeño nos puede hacer perder la oportunidad de ser mucho más rápidos, pero un paso demasiado grande, el mínimo de la función modelo $m_k$ puede estar lejos del mínimo de la función objetivo. Este último caso es el que se comprueba y se decide si reducir la región de confianza.

Una vez tomado el radio, encontrar el mínimo es directo en el caso de que $B_k$ sea definida positiva, basta tomar $p_k^B = -B_k^{-1}g_k$. En caso contrario tampoco supone una tarea muy costosa ya que sólo se necesita una solución aproximada para garantizar la convergencia.

\chapter{Mínimos Cuadrados}
%campos en los que se utiliza??? 

En el problema de mínimos cuadrados la función objetivo es de la forma
\begin{equation}
	f(x) = \frac{1}{2} r(x)^Tr(x) = \frac{1}{2}\sum_{i=1}^{m}r_i^2(x),
	\label{eq:lsp}
\end{equation}
donde $r(x) = (r_1(x), r_2(x), \ldots, r_m(x))^T$, con $r_i : \mathbb{R}^{n} \rightarrow \mathbb{R}$ funciones continuamente diferenciables, llamado \textit{residuo}.
Se asume que $m \geq n$ para este problema, de hecho en la práctica lo más común es que $m \gg n$. 

Veamos las propiedades de este modelo concreto de optimización sin restricciones y cómo se pueden aprovechar para formular algoritmos eficientes y robustos.
Sea $J(x)$ la matriz Jacobiana de $r(x)$, 
\begin{equation}
	J(x) = 
	\begin{bmatrix}
		\nabla r_1(x)^T \\
		\nabla r_2(x)^T \\
		\vdots \\
		\nabla r_m(x)^T
	\end{bmatrix}
	=
	\begin{bmatrix}
		\frac{\partial r_1}{\partial x_1}(x) &
		\frac{\partial r_1}{\partial x_2}(x) &
		\cdots &
		\frac{\partial r_1}{\partial x_n}(x) \\
		
		\frac{\partial r_2}{\partial x_1}(x) &
		\frac{\partial r_2}{\partial x_2}(x) &
		\cdots &
		\frac{\partial r_2}{\partial x_n}(x) \\
		
		\cdots &
		\cdots &
		\cdots &
		\cdots & \\
		
		\frac{\partial r_m}{\partial x_1}(x) &
		\frac{\partial r_m}{\partial x_2}(x) &
		\cdots &
		\frac{\partial r_m}{\partial x_n}(x) 
	\end{bmatrix}.
\end{equation}
El gradiente y la Hessiana de f se pueden expresar como sigue:
\begin{align}
	g(x) = \nabla f(x) &= \sum_{i=1}^m r_i(x) \nabla r_i(x) = J(x)^Tr(x) \label{eq:grad}\\
	G(x) = \nabla^2 f(x) &= \sum_{i=1}^m \nabla r_i(x) \nabla r_i(x)^T + \sum_{i=1}^m r_i(x)\nabla^2r_i(x) \nonumber \\
	&= J(x)^TJ(x)+S(x). \label{eq:hess}
\end{align}

Si nos fijamos en la formulación de la matriz Hessiana, el cálculo del primer termino es directo gracias a que ya obtenemos $J(x)$ para calcular el gradiente (\ref{eq:grad}), así que el coste se reduce al segundo término, que hemos denotado $S(x)$.
Veamos la expresión del modelo cuadrático de $f(x)$ utilizando (\ref{eq:lsp}), (\ref{eq:grad}) y (\ref{eq:hess}):
\begin{equation}
\begin{split}
	q^{(k)}(x) &= f(x_k) + g^T_k(x-x_k) + \frac{1}{2}(x-x_k)^TG_k(x-x_k) \\
	&= \frac{1}{2}r(x_k)^Tr(x_k) + (J(x_k)^Tr(x))^T(x-x_k) +
	
\end{split}
\end{equation}


%%%%%%%%     Póñense a bibliografía, Apéndices e o índice alfabético     %%%%%%%%
\appendix
\renewcommand{\thechapter}{\Roman{chapter}}
\chapter{Título del Anexo I}


\chapter{Título del Anexo II}

\backmatter

\begin{thebibliography}{99}

%%%%%%%%----Exemplo de entradas bibliográficas:
%


\bibitem{Noc} Nocedal, J., \& Wright, S. (2006). \emph{Numerical Optimization} (2nd ed.). Springer.

\bibitem{Sun} Sun, W., \& Yuan, Y.-X. (2006). \emph{Optimization theory and methods: Nonlinear programming} (2006th ed.). Springer.

%
\end{thebibliography}




\end{document}


