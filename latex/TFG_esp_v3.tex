 %%%%%%%%%%     PREÁMBULO      %%%%%%%%%%%%%%%

%%%%%%%%%   Non modificar este preámbulo, salvo que se sepa ben o que se está a facer   %%%%%%%%%

%%%%%%  Traballando nun Mac, se usas TeXShop, en  Preferencias -> Código -> codificación de  caracteres (UTF-8)  %%%%

\documentclass[11pt,a4paper]{book}

% Formato do documento
\usepackage[papersize={210mm,297mm},lmargin=2.5cm,rmargin=2.5cm,top=3.5cm,bottom=3.5cm]{geometry}   % Marxes
\renewcommand{\baselinestretch}{1.25}                                                               % Fixamos o interliñado
\setlength{\parskip}{8pt}                                                                           % Separación entre parágrafos


\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{latexsym,pdfsync,xcolor,graphicx}
\usepackage[nottoc]{tocbibind}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}			
\usepackage{tikz}
\usepackage{appendix}
\usepackage{hyperref}



\usepackage{algpseudocode}
\usepackage{enumitem}
\newlist{steps}{enumerate}{1}
\setlist[steps, 1]{label = Paso \arabic*., leftmargin=2.6cm, topsep=0pt}



% Cabeceira
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{\textbf{\thechapter. #1}}{}} % Formato para o capítulo: N. Nome
\renewcommand{\sectionmark}[1]{\markright{\textbf{\thesection. #1}}}  % Formato para a sección: N.M. Nome

\fancyhead[LO]{\rightmark}           % Nas páxinas impares, parte esquerda do encabezado, aparecerá o nome do capítulo
\fancyhead[RE]{\leftmark}            % Nas páxinas pares, parte dereita do encabezado, aparecerá o nombre da sección
\fancyhead[RO,LE]{\textbf{\thepage}} % Números de páxina nas esquinas dos encabezados




%%%   \pagestyle{plain}   Cando os títulos dos capítulos son moi longos, a opción por defecto das cabeceiras pode ser inapropiada;
%%%  nestes e noutros casos semellantes, pódese usar a opción "plain", que suprime as cabeceiras, deixando soamente a numeración das páxinas.


%%%%%%%      NON ESCRIBIR AQUÍ  (COMANDO PORTADA) %%%%%%%%%%%

\newcommand{\portada}[3]{
\thispagestyle{empty}

\definecolor{azulUSC}{RGB}{13,38,120}

\includegraphics[width=13cm]{logomath.pdf}

\vspace*{2cm}

\centerline{{\Large\bf  Traballo Fin de Grao}}

\vspace{2.6cm}

{\center{\Huge\bfseries\color{azulUSC} #1}\par}		

\vspace{1cm}

{\Large

\centerline{#2}		

\vspace{6.4cm}

\centerline{\sf #3}        

\vspace{1.25cm}

\centerline{\sf UNIVERSIDADE DE SANTIAGO DE COMPOSTELA}
}

\enlargethispage{1cm}
\clearpage
}

%%%%%%%      NON ESCRIBIR AQUÍ  (COMANDO PRIMEIRA PÁXINA) %%%%%%%%%%%

\newcommand{\primeira}[3]{
\thispagestyle{empty}
\enlargethispage{1cm}
\begin{center}\Large

{\sf GRAO DE MATEM\'ATICAS}

\bigskip
{\bfseries Traballo Fin de Grao}
\vspace{4cm}

{\bfseries\Huge #1} 		
\vspace{1.2cm}

{#2}		\par		     
\end{center}

\vspace{6.5cm}

\begin{center}
\Large
{#3}	\par			    

\vspace{1cm}
{\sf UNIVERSIDADE DE SANTIAGO DE COMPOSTELA}
\end{center}
\clearpage
}

%%%%%%%    Teoremas   %%%%%%%%%

\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{corollary}[theorem]{Corolario}
\newtheorem{algorithm}[theorem]{Algoritmo}
\newtheorem{axiom}[theorem]{Axioma}
\newtheorem{case}[theorem]{Caso}
\newtheorem{conclusion}[theorem]{Conclusión}
\newtheorem{condition}[theorem]{Condición}
\newtheorem{conjetura}[theorem]{Conjetura}				
\newtheorem{criterion}[theorem]{Criterio}
\newtheorem{ejercicio}[theorem]{Ejercicio}	
			
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definición}
\newtheorem{ejemplo}[theorem]{Ejemplo}				
\newtheorem{agradecimientos}[theorem]{Agradecimientos}	

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Observación}
\newtheorem{notation}[theorem]{Notación}
\newtheorem{problem}[theorem]{Problema}
\newtheorem{solution}[theorem]{Solución}

\addto\captionsspanish{
	\renewcommand{\contentsname}{\'Indice} 
	\renewcommand\appendixname{Anexo}
	\renewcommand\appendixpagename{Anexos}
}


%%%%%%%%%%%%%%%%%%   Fin do Preámbulo   %%%%%%%%%%%%%%%%

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\sucesionxk}{\left\{x_k\right\}}
\newcommand{\sucesion}[1]{\left\{#1\right\}}


\begin{document}
\frontmatter

%%%%%%%%%%%      PORTADA      %%%%%%%%%%%%

\portada
{Resolución numérica del problema no lineal de mínimos cuadrados.\vspace{5pt}
Aplicaciones a la estimación de parámetros de modelos matemáticos.}		%  escribir aquí o  Título
{Dídac Blanco Morros}		%  escribir aquí o/a  Autor/a
{Curso Acad\'emico}		  	%  escribir aquí a Data de presentación, na forma:  mes, ano

%%%%%%%%     PRIMEIRA PÁXINA    %%%%%%%%%%%%%%%%%%

\mbox{}
\thispagestyle{empty}
\clearpage

\setcounter{page}{1}

\primeira
{Resolución numérica del problema no lineal de mínimos cuadrados. 
Aplicaciones a la estimación de parámetros de modelos matemáticos.}		%  escribir aquí o  Título
{Dídac Blanco Morros}		%  escribir aquí o/a  Autor/a
{Febrero, 2022}		  	%  escribir aquí a Data de presentación, na forma:  mes, ano

\thispagestyle{empty}

%%%%%%%%%%%      PROPOSTA DE TRABALLO      %%%%%%%%%%%%%%%%

\vspace*{.5 cm}

\chapter*{Trabajo propuesto}

\vspace{1 cm}

\begingroup
\renewcommand*{\arraystretch}{2.2}
\begin{tabular}{|l|}
	\hline
	
	{\bf \'Area de Co\~necemento:  \/ Matemática Aplicada}\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}
			\bf T\'{\i}tulo:   \/ \bf Resolución numérica do problema non linear de mínimos cadrados. Aplicacións á estimación de parámetros de modelos matemáticos.
			\vspace{.2cm}}
	\end{minipage}\\ \hline
	\bf Breve descrici\'on do contido\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}
O problema non linear de mínimos cadrados zurde en moutas aplicacións da ciencia e da enxeñería: no axuste dun conxunto de datos a un modelo matemático, na estimación de parámetros, na aproximación de funcións, etc. O obxectivo do traballo fin de grao é o estudo de métodos numéricos para abordar o problema de minimización resultante. centrándose especialmente no algoritmo de Levenberg-Marquardt. O estudante estudará o método, no marco dos métodos de optimización con rexión de confianza e familiarizarase co seu uso mediante o comando Isqnonlin de Matlab. As metodoloxías estudadas aplicaranse a exemplos académicos e á estimación de parámetros de distintos modelos matemáticos a partir de datos experimentais.
		
		\vspace{.2cm}}
	\end{minipage}\\ \hline
	{\bf Recomendaci\'ons}\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}


		\vspace{.2cm}}
	\end{minipage}\\ \hline
	{\bf Outras observaci\'ons}\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}


			\vspace{.2cm}}
	\end{minipage}\\ \hline
\end{tabular}
\endgroup

\clearpage

\thispagestyle{empty}

\tableofcontents

\clearpage

\thispagestyle{empty}

\mbox{}

\clearpage

%%%%%%%%%     RESUMO---RESUMEN---ABSTRACT     %%%%%%%%%%%%%

\phantomsection
\addcontentsline{toc}{chapter}{Resumen}		
\chapter*{}

\section*{Resumen}

% escribir aquí resumo en español




\vspace{1.5cm}

\section*{Abstract}



% escribir aquí resumo en inglés



\clearpage

\thispagestyle{empty}

%%%%%%%%%    INTRODUCIÓN     %%%%%%%%%%%%%%%%%%
%%%%%%%%%    De non incluír unha INTRODUCIÓN, suprímese    %%%%%%%%%%

\phantomsection
\chapter*{Introducci\'on}\addcontentsline{toc}{chapter}{Introducci\'on}

\markboth{INTRODUCCI\'ON}{INTRODUCCI\'ON}

%%%%%%%%%%%     fin da INTRODUCIÓN     %%%%%%%%%%%%%

\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				S
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				 T
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				  A
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				   R
%%%%%%%%     Póñense os Capítulos    %%%%%%%%%%%%%%%				    T



\chapter{Fundamentos de la optimización sin restricciones}
% El problema de los mínimos cuadrados es un caso particular de optimización sin restricciones, y es por ello que comenzaremos introduciendo sus fundamentos. Ya que el problema de mínimos cuadrados es usado en multitud de campos para estimar parámetros, este es de los más utilizados dentro de los problemas de optimización sin restricciones.

Un problema de optimización sin restricciones tiene la forma 
\begin{equation}
	\min_{x}f\left(x\right),
	\label{eq:minf}
\end{equation}
donde $x\in\mathbb{R}^{n}$ y $f : \mathbb{R}^{n} \rightarrow \mathbb{R}$ es continuamente diferenciable, la llamamos \textbf{función objetivo}. 
Notar que podemos usar esta formulación para referirnos tanto a los problemas de minimización como de maximización, basta sustituir $f$ por $-f$. 

La dificultad de un problema como este viene de no conocer el comportamiento global de $f$, normalmente solo disponemos de la evaluación de f en algunos puntos, y a lo mejor de algunas de sus derivadas. El trabajo de los algoritmos de optimización es identificar la solución sin usar demasiado tiempo ni almacenamiento computacional.

% W. Sun es más formal en este caso para definir ambos mínimos 
\begin{definition}
	A una aplicación $\Vert \cdot \Vert$ se le llama \textit{norma} si y sólo si cumple:
	\vspace{-0.4cm}
	\begin{enumerate}
		\item $\norm{x} \geq 0, \: \forall \mathbb{R}^n$ y $\norm{x}=0$ si y solo si $x=0$.
		\item $\norm{\alpha x} = \vert \alpha \vert \norm{x},\: \forall \alpha \in \mathbb{R},\: x\in \mathbb{R}^n.$
		\item $\norm{x+y} \leq \norm{x} + \norm{y}, \forall x,y \in \mathbb{R}^n$.
	\end{enumerate}
\end{definition}

Un ejemplo muy común es la
\textit{norma} $l_2$, también llamada \textit{norma euclídea}, a la cual nos referiremos cuando no se especifique lo contrario, se define
\begin{equation}
		\norm{x}_2 = \left( \sum_{i=1}^n \vert x_i \vert^2 \right)^{\frac{1}{2}}.
\end{equation}

\begin{definition}
	Un punto $x^*$ se dice \textit{mínimo local} si existe $\delta > 0$ tal que $f(x^*) \leq f(x)$ para todo $x \in \mathbb{R}^n$ que satisface $\Vert x - x^* \Vert < \delta$.
	Un punto $x^*$ se dice \textit{mínimo local estricto} si existe $\delta > 0$ tal que $f(x^*) < f(x)$ para todo $x \in \mathbb{R}^n$ que satisface $\Vert x - x^* \Vert < \delta$ con $x \neq x^*$.
\end{definition}

\begin{definition}
	Un punto $x^*$ se dice \textit{mínimo global} si $f(x^*) \leq f(x)$ para todo $x \in \mathbb{R}^n$.
	Un punto $x^*$ se dice \textit{mínimo global estricto} si $f(x^*) < f(x)$ para todo $x \in \mathbb{R}^n$ con $x \neq x^*$.
\end{definition}

Como no se suele tener un conocimiento a gran escala de $f$ debido a su coste, la mayoría de algoritmos solo encuentran mínimos locales, lo cual es suficiente para muchos casos prácticos.

Aún así, los algoritmos para encontrar mínimos globales se suelen construír a partir de una secuencia de otros algoritmos de optimización local. También podemos aprovechar características fáciles de detectar en la función objetivo, como la convexidad, que nos asegura que un mínimo local será también global.

%% REPASO DE ALGORITMOS

%% \section{Generalidades de los algoritmos}

%% histórico??
% referencia 	NOCEDAL 2.2

\begin{definition}
	Sea $f: \mathbb{R}^n \rightarrow \mathbb{R}$ diferenciable en
$x \in \mathbb{R}^n$ tal que $\langle \nabla f(x), d \rangle < 0$, entonces a $d$ se le llama \textit{dirección descendente} de $f$ en $x$.
\end{definition}

\begin{theorem}[Teorema de Taylor] %% COMPATIBILIZAR CON RESULTADO ANTERIOR
	
	Sea $f: \mathbb{R}^n \rightarrow \mathbb{R}$ continuamente diferenciable y sea $p \in \mathbb{R}^n$, tenemos que 
	\begin{equation}
		f(x+p) = f(x) + \nabla f(x+tp)^Tp, \quad t\in (0,1).
	\end{equation}
	Si además, $f$ es dos veces continuamente diferenciable
	\begin{equation}
		\nabla f(x+p) = \nabla f(x)
		+ \int_0^1 \nabla^2 f(x+tp)p\:dt,
	\end{equation}
	y
	\begin{equation}
		f(x+p) = f(x) + \nabla f(x)^Tp
		+ \frac{1}{2}p^T \nabla^2 f(x+tp)p, \; t\in (0,1).
		\label{eq:thty}
	\end{equation}
\end{theorem}


\begin{proposition}
Partiendo de la reformulación de (\ref{eq:thty}) y teniendo en cuenta que el último término es el error de aproximación $o(t)$
\begin{equation}
	f(x_k + td) = f(x_k) + t \nabla f(x_k)^Td + o(t),
\end{equation}
se cumple que
\begin{equation}
	\exists \delta > 0 \text{ tal que } f(x_k + td) < f(x_k)
	\quad \forall t \in (0, \delta)
\end{equation}
si y solo si $d$ es una dirección descendiente de $f$ en $x_k$.

\end{proposition}

Tratemos ahora las condiciones de optimalidad.

\begin{theorem}[Condición Necesaria de Primer Orden]
	Sea $f:D\subset \mathbb{R}^n \rightarrow \mathbb{R}$ continuamente diferenciable en un conjunto abierto $D$. Si $x^*$ es un mínimo local de (\ref{eq:minf}), entonces $\nabla f(x^*) = 0$.
\end{theorem}

\begin{theorem}
	(Condición Necesaria de Segundo Orden)
	Sea $f:D\subset \mathbb{R}^n \rightarrow \mathbb{R}$ dos veces continuamente diferenciable en un conjunto abierto $D$. Si $x^*$ es un mínimo local de (\ref{eq:minf}), entonces $\nabla f(x^*) = 0$ y $\nabla^2 f(x^*)$ es definida positiva.
\end{theorem}

\begin{theorem}[Condición Suficiente de Segundo Orden]
	Sea $f:D\subset \mathbb{R}^n \rightarrow \mathbb{R}$ dos veces continuamente diferenciable en un conjunto abierto $D$. Si $\nabla f(x^*) = 0$ y $\nabla^2 f(x^*)$ es definida positiva, entonces $x^* \in D$ es un mínimo local.
\end{theorem}

Para poder dar un último resultado para soluciones óptimas en minimización, veamos unas últimas definiciones.

\begin{definition}
	Sea $S\subset \mathbb{R}^n$ y sean $x_1, x_2 \in S$ cualesquiera. Si
	$\alpha x_1 + (1 - \alpha)x_2 \in S$ para todo $\alpha \in [0,1]$, entonces se dice que D es un \textit{conjunto convexo}.
\end{definition}
\begin{definition}
	Sea $S \subset \mathbb{R}^n$ un conjunto convexo no vacío. Sea $f: S \subset \mathbb{R}^n \rightarrow R$. Si para cualquiera $x_1, x_2 \in S$ y
	$\alpha \in (0,1)$, se cumple que
	\begin{equation}
		f(\alpha x_1 + (1-\alpha)x_2) \leq \alpha f(x_1) + (1-\alpha)f(x_2),
	\end{equation}
	se dice que $f$ es una función convexa en $S$.
\end{definition}

\begin{theorem}
	Sea $S \subset \mathbb{R}^n$ un conjunto convexo no vacío y
	$f:S \subset \mathbb{R}^n \rightarrow \mathbb{R}$ una función convexa. Si $x^*$ es mínimo local de (\ref{eq:minf}), entonces también es mínimo global.
\end{theorem}

\begin{theorem} \label{th:convx}
	Sea $f: \mathbb{R}^n \rightarrow \mathbb{R}$ convexa y diferenciable, entonces $x^*$ es un mínimo global si y solo si $\nabla f(x^*) = 0$.
\end{theorem}

Todo algoritmo de optimización sin restricciones comienza con un punto de partida, denotado normalmente como $x_{0}$. Aunque generalmente el usuario introduce una estimación razonable, el punto puede ser elegido por el algoritmo, tanto de forma sistemática como aleatoria. El algoritmo itera sobre $x_{0}$, creando una sucesión $\sucesionxk_{k=0}^n$ la cual termina cuando no pueda continuar o cuando ya se haya acercado razonablemente a la solución. Para decidir como se avanza de un $x_k$ al siguiente, los algoritmos utilizan información sobre $f(x_k)$ o incluso en los puntos anteriores $x_0,\, x_1,\, \ldots,\,x_{k-1}$ con el objetivo de que $f(x_{k+1})<f(x_{k})$. Hablaremos de las dos estrategias fundamentales que se utilizan para avanzar de $x_k$ a $x_{k+1}$, \textit{búsqueda de línea} y \textit{región de confianza}.

\section{Búsqueda de línea}

En este caso el algoritmo tiene dos tareas a partir de cada iteración, primero elige una \textit{dirección} $d_k$ y tomando el punto de partida busca en esa dirección el nuevo valor. Es decir, dado $x_k$
\begin{equation}
	\label{eq:linesearch}
	x_{k+1} = x_k + \alpha_kd_k
\end{equation}
para un $d_k$ elegido previamente, y un \textit{paso} $\alpha_k$ obtenido solucionando otro problema de minimización más simple por ser unidimensional:
\begin{equation}
	\min_{\alpha_k>0}f\left(x_k+\alpha_kd_k\right).
	\label{min:alphak}
\end{equation}

Si se toma el $\alpha_k$ óptimo se le llama búsqueda de línea \textit{exacta} u \textit{óptima}. Para evitar el gran coste computacional que puede llegar a tomar, lo más común es tomar un $\alpha_k$ que aporte un descenso aceptable, en cuyo caso se le llama búsqueda de línea \textit{inexacta} o \textit{aproximada}. Desde el nuevo punto se busca otra dirección y paso para repetir el proceso.
Veamos brevemente cómo se eligen $d_k$ y $\alpha_k$.

La mayor parte de algoritmos de este tipo necesitan que $d_k$ sea una dirección descendente, esto es, $d_k^T \nabla f_k < 0$,
lo cual asegura que en esa dirección se podrá reducir el valor de $f$. Esta suele tener la forma
\begin{equation}
	d_k = -B_k^{-1} \nabla f_k
\end{equation}
con $B_k$ una aproximación de la matriz Hessiana $\nabla^2 f(x_k)$ simétrica y no singular. Según lo que acabamos de decir, necesitamos que $B_k$ sea definida positiva. En las tres corrientes principales se elige un $B_k$ distinto, en el \textit{método del descenso máximo} o \textit{descenso del gradiente}, se usa la matriz identidad $I$.
En el \textit{método de Newton} se usa la matriz exacta, mientras que en los \textit{métodos Quasi-Newton} la matriz Hessiana es aproximada para cada $x_k$.

En el caso de la elección de $\alpha_k$, el caso ideal sería encontrar el óptimo en \ref{min:alphak}, pero esto es en general demasiado costoso.
Debido a ese coste, se suelen utilizar búsquedas inexactas probando una serie de puntos hasta que alguno cumpla unas condiciones preestablecidas con las que se acepta el paso dado.
Estas condiciones son por ejemplo las condiciones \textit{Wolfe} o las condiciones \textit{Goldstein}.
Esta elección se hace en dos fases, primero un proceso elige un intervalo conteniendo los pasos deseables y una segunda fase donde se va reduciendo el intervalo por técnicas de interpolación o bisección. % el mejor?

\subsection{Método de Newton}
Veamos brevemente las ideas detrás del método de Newton, influyentes en los demás planteamientos. La idea principal es usar la aproximación cuadrática $q^{(k)}$ de la función objetivo,
\begin{equation}
	q^{(k)}(p) = f(x_k)+\nabla f(x_k)^Tp + \frac{1}{2}p^T \nabla^2 f(x_k)p,
	\label{eq:NewtonQ}
\end{equation}
si $f:\mathbb{R}^n \rightarrow \mathbb{R}$ dos veces continuamente diferenciable, $x_k \in \mathbb{R}^n$ y la Hessiana $\nabla^2f(x_k)$ es definida positiva. En tal caso aproximamos $f(x_k + p) \approx q^{(k)}(p)$.

Minimizando $q^{(k)}(p)$ obtenemos la fórmula de Newton, denotando $G_k=\nabla^2f(x_k)$ y $g_k = \nabla f(x_k)$:
\begin{equation}
	x_{k+1} = x_k - G_k^{-1} g_k.
	\label{eq:NewtonIter}
\end{equation}

\begin{theorem}[Teorema de Convergencia del Método de Newton]
	Sea
	$f \in \mathcal{C}^2$ y $x_k$ lo suficientemente cerca a
	la solución $x^*$ del problema de minimización con
	$g(x^*)=0$. Si la Hessiana $G(x^*)$ es definida positiva y
	$G(x)$ satisface la condición de Lipschitz
	\begin{equation}
		\vert G_{ij}(x) - G_{ij}(y) \vert \leq 
		\beta \Vert x-y \Vert,
		\text{ para algún } \beta,
		\text{ y para todo } i,j,
	\end{equation}
	siendo $G_{ij}(x)$ el elemento en la posición $(i,j)$ de la matriz
	$G(x)$, entonces para todo $k$, la iteración (\ref{eq:NewtonIter})
	está bien definida y la sucesión $\left\{ x_k \right\}$
	generada converge a $x^*$ de forma cuadrática.
\end{theorem}

\section{Región de confianza}

Esta estrategia enfoca el problema de otro modo, primero se fija una distancia máxima $\Delta_k$ para definir una región, generalmente de la forma
\begin{equation}
	\Omega_k = \left\{x : \Vert x-x_k \Vert \leq \Delta_k \right\}
\end{equation}
y luego ya se busca la dirección y paso.
A partir de la información conocida de $f$, para cada $x_k$ se modela una función $m_k$ que se comporte de manera similar a $f$ cerca de este punto.
Se suele utilizar el modelo cuadrático $q^{(k)}$ visto anteriormente (\ref{eq:NewtonQ}), aprovechando la notación usada en el apartado anterior:
\begin{equation}
	m_k(p) := q^{(k)}(p) = f(x_k) + g^T_kp + \frac{1}{2}p^TG_kp.
\end{equation}

Como hemos visto, este modelo se utiliza en los métodos de búsqueda de línea para determinar la dirección de búsqueda, mientras que en este caso lo usamos para tener una representación adecuada de la función objetivo y así elegir el mínimo dentro de esta región.
Este método nos evita el problema de que la Hessiana no sea definida positiva.
En cada iteración, una vez elegido $\Delta_k$ se resuelve el siguiente problema:
\begin{equation}
\label{min:tr}
\begin{aligned}
	\min_{p} \quad & m_k(p) = f(x_k) + g^T_kp + \frac{1}{2}p^TB_kp \\
	\text{s.a.} \quad & \Vert p \Vert \leq \Delta_k. \\
\end{aligned}
\end{equation}
Notamos que en el modelo se escribe $B_k$ en lugar de $G_k$, pues no siempre se usa esta última.
Debido al coste computacional, como vimos en la elección de la dirección de búsqueda, a veces se prefiere aproximar de alguna manera más o menos eficiente, e incluso puede ser aceptable tomar la matriz $0$.

También se puede elegir qué norma define la región de confianza, cambiando así la forma de esta y ofreciendo distintos resultados, aunque generalmente se utiliza la bola definida por $\Vert p \Vert_2 \leq \Delta_k$.

La efectividad de cada iteración depende de la elección del radio $\Delta_k$, es por ello que puede que la primera elección de este no sea la definitiva. Es decir, se toma un radio a raíz de la información que se tenga, esta puede incluír la de pasos anteriores, y luego se decide si este radio nos da un resultado aceptable. Un radio demasiado pequeño nos puede hacer perder la oportunidad de ser mucho más rápidos, pero un paso demasiado grande, el mínimo de la función modelo $m_k$ puede estar lejos del mínimo de la función objetivo. Este último caso es el que se comprueba y se decide si reducir la región de confianza.

Una vez tomado el radio, encontrar el mínimo es directo en el caso de que $B_k$ sea definida positiva, basta tomar $p_k^B = -B_k^{-1}g_k$, conocido como \textit{paso completo}. En caso contrario tampoco supone una tarea muy costosa ya que sólo se necesita una solución aproximada para garantizar la convergencia.

Veamos ahora de forma detallada como se elige el radio $\Delta_k$ en cada iteración. Esta elección se toma según el parecido entre la función $f$ y el modelo $m_k$ tomado en las iteraciones previas. Dado $p_k$, definimos el ratio
\begin{equation}
\label{eq:rho_k}
	\rho_k = \frac{f(x_k)-f(x_k+p_k)}{m_k(0)-m_k(p_k)},
\end{equation}
donde el numerador es la \textit{reducción real}, mientras que el denominador es la \textit{reducción prevista}.
La reducción prevista será positiva por definición, pues $p_k$ es elegido para tener el menor valor posible y el $0$ es una posibilidad.
Por tanto, si $\rho_k$ es negativo, el nuevo valor $f(x_k+p_k)$ no es menor que $f(x_k)$ y este paso ha de ser rechazado.
Por otro lado, si $\rho_k$ es cercano a $1$, esto quiere decir que $f$ y $m_k$ se comportan de manera similar en la región tomada en la iteración actual, por tanto podemos agrandar el radio con seguridad.
En resumen, nos quedamos con el radio elegido si $\rho_k$ no tiene un valor muy cercano a $0$ o a $1$.
El proceso se describe en el siguiente algoritmo.
\begin{algorithm}[Región de confianza]\leavevmode
\begin{algorithmic}[1]
\State Dado $\hat{\Delta} > 0,\Delta_0 \in (0,\hat{\Delta}),$ y $\eta \in [0,\frac{1}{4})$
\For{$k \gets 0,1,2,\ldots$}
    \State Obtener $p_k$ (\ref{min:tr}).
    \State Calcular $\rho_k$ (\ref{eq:rho_k}).
    \If{$\rho_k < \frac{1}{4}$}
        \State $\Delta_{k+1} \gets \frac{1}{4}\Delta_k$
    \Else
        \If{$\rho_k > \frac{3}{4}$ y $\norm{p_k} = \Delta_k$}
            \State $\Delta_{k+1} \gets \min(2\Delta_k,\hat{\Delta})$
        \Else
            \State $\Delta_{k+1} \gets \Delta_k$
        \EndIf
    \EndIf
    \If{$\rho_k > \eta$}
        \State $x_{k+1} \gets x_k + p_k$
    \Else
        \State $x_{k+1} \gets x_k$
    \EndIf
\EndFor.
\end{algorithmic}
\end{algorithm}
Aquí $\hat \Delta$ es el máximo radio de la región de cada iteración.
Notar que únicamente se aumenta el radio en el caso de que $\norm{p_k} = \Delta_k$, ya en caso contrario, se entiende que el $\Delta_k$ elegido no influye en la elección de forma negativa.

Para que este algoritmo sea práctico, nos centramos en la resolución del subproblema (\ref{min:tr}). Tomemos una notación más limpia,
\begin{equation}
\label{min:tr_nk}
\begin{aligned}
	\min_{p} \quad & m(p) = f + g^Tp + \frac{1}{2}p^TBp, \\
	\text{s.a.} \quad & \Vert p \Vert \leq \Delta. \\
\end{aligned}
\end{equation}

\begin{theorem} \label{th:trustregion}
	El vector $p^*$ es una solución global de (\ref{min:tr_nk}) si y solo si $p^*$ cumple las condiciones y existe un escalar $\lambda \geq 0$ tal que se cumplen las siguientes condiciones:
\begin{enumerate}
%\vspace{-0.3cm}
	\item $(B+\lambda I)p^*=-g,$
	\item $\lambda (\Delta - \norm{p^*}) = 0,$
	\item $(B+\lambda I)$ es semidefinida positiva.
\end{enumerate}
\end{theorem}
\begin{proof}
página 90 Nocedal.
\end{proof}

Este teorema caracteriza la solución según el primer punto. El segundo punto es una condición complementaria que nos dice que al menos uno de los dos factores es 0.
Esto es, si $\norm{p}<0$, $\lambda$ tendrá que ser 0 y $Bp^*=-g$ con $B$ definida positiva (puntos 1 y 3).
En el caso de que $\norm{p}$ se maximice, lo cual da a entender que la solución óptima no se encuentra dentro de la región de confianza, $\lambda$ podrá tomar valores positivos.

\chapter{Mínimos Cuadrados}
%campos en los que se utiliza??? 

El problema de mínimos cuadrados surge de la necesidad de ajustar modelos que nos permitan predecir ciertos comportamientos en una amplia variedad de campos. Dados unos datos $(t_1,y_1),(t_2,y_2),\cdots,(t_m,y_m)$, queremos ajustar una función
$\phi(t,x)$ de forma que se minimicen los residuos $r_i(x) = \phi(t_i,x) - y_i$ para $i=1,\cdots,m$
\begin{equation}
	\min_{x\in \mathbb{R}^{n}}f(x) = \frac{1}{2} r(x)^Tr(x) = \frac{1}{2}\sum_{i=1}^{m}r_i^2(x), \quad m\geq n,
	\label{eq:lsp}
\end{equation}
donde $r(x) = (r_1(x), r_2(x), \ldots, r_m(x))^T$, con $r_i : \mathbb{R}^{n} \rightarrow \mathbb{R}$ funciones continuamente diferenciables. 

%Se asume que $m \geq n$ para este problema, de hecho en la práctica lo más común es que $m \gg n$. 

Veamos las propiedades de este modelo concreto de optimización sin restricciones y cómo se pueden aprovechar para formular algoritmos eficientes y robustos.
Sea $J(x)$ la matriz Jacobiana de $r(x)$, 
\begin{equation}
	J(x) = 
	\begin{bmatrix}
		\nabla r_1(x)^T \\
		\nabla r_2(x)^T \\
		\vdots \\
		\nabla r_m(x)^T
	\end{bmatrix}
	=
	\begin{bmatrix}
		\frac{\partial r_1}{\partial x_1}(x) &
		\frac{\partial r_1}{\partial x_2}(x) &
		\cdots &
		\frac{\partial r_1}{\partial x_n}(x) \\
		
		\frac{\partial r_2}{\partial x_1}(x) &
		\frac{\partial r_2}{\partial x_2}(x) &
		\cdots &
		\frac{\partial r_2}{\partial x_n}(x) \\
		
		\cdots &
		\cdots &
		\cdots &
		\cdots & \\
		
		\frac{\partial r_m}{\partial x_1}(x) &
		\frac{\partial r_m}{\partial x_2}(x) &
		\cdots &
		\frac{\partial r_m}{\partial x_n}(x) 
	\end{bmatrix}.
\end{equation}
El gradiente y la Hessiana de $f$ se pueden expresar como sigue:
\begin{align}
	g(x) = \nabla f(x) &= \sum_{i=1}^m r_i(x) \nabla r_i(x) = J(x)^Tr(x), \label{eq:grad}\\
	G(x) = \nabla^2 f(x) &= \sum_{i=1}^m \nabla r_i(x) \nabla r_i(x)^T + \sum_{i=1}^m r_i(x)\nabla^2r_i(x) \nonumber \\
	&= J(x)^TJ(x)+S(x). \label{eq:hess}
\end{align}

Si nos fijamos en la formulación de la matriz Hessiana, el cálculo del primer termino es directo gracias a que ya obtenemos $J(x)$ para calcular el gradiente (\ref{eq:grad}), así que el coste se reduce al segundo término, que hemos denotado $S(x)$.
Adaptando el modelo cuadrático (\ref{eq:NewtonQ})
%Veamos la expresión del modelo cuadrático de $f(x)$ utilizando (\ref{eq:lsp}), (\ref{eq:grad}) y (\ref{eq:hess}):
\begin{equation}
	q^{(k)}(x) = f(x_k) + g^T_k(x-x_k) + \frac{1}{2}(x-x_k)^TG_k(x-x_k),
	\label{eq:q}
\end{equation}
y sustituyendo según (\ref{eq:lsp}), (\ref{eq:grad}) y (\ref{eq:hess}), obtenemos el método de Newton para (\ref{eq:lsp}),
\begin{equation}
	x_{k+1} = x_k-(J(x_k)^TJ(x_k)+S(x_k))^{-1}J(x_k)^Tr(x_k).
	\label{eq:newtoniter}
\end{equation}

\section{El Problema Lineal}

El primer caso más sencillo es si $\phi (t, x)$ es una función lineal, en cuyo caso los residuos
$r_i(x)$ también serán lineales.
Por ser $\phi$ lineal, se puede representar como $Jx$, con $J$ una matriz
$m\times n$.
Realizaremos un estudio del caso lineal para tener un conocimiento de como se enfocan estos problemas, que nos servirá para entender mejor el caso no lineal.
Si escribimos el vector residuo como $r(x) = Jx-y$, la función objetivo nos queda de la forma
\begin{equation}
	f(x) = \frac{1}{2} \Vert Jx-y \Vert^2.
	\label{eq:flin}
\end{equation}
En consecuencia, tomando como referencia (\ref{eq:grad}) y (\ref{eq:hess}) y teniendo en cuenta que en este caso particular $\nabla^2r_i=0$, nos queda
\begin{equation}
	\nabla f(x) = J^T(Jx-y), \qquad \nabla^2 f(x) =  J^TJ.
\end{equation}

Como $f$ es convexa, dado un punto $x^*$ tal que $\nabla f(x^*) = 0$, este será mínimo global (\ref{th:convx}). Por tanto, $x^*$ satisface el siguiente sistema lineal:
\begin{equation}
	J^TJx^* = J^Ty.
\end{equation}

Antes de ver como se resuelve numéricamente este sistema de ecuaciones, conocidas como \textit{ecuaciones normales} de (\ref{eq:flin}), veamos los conceptos previos necesarios.

\begin{definition}
	Un problema numérico se dice \textit{bien condicionado} si su solución no se ve afectada por pequeñas perturbaciones a cualquiera de los datos que definen el problema.
\end{definition}

\begin{definition}
	Una matriz cuadrada $A$ se dice \textit{definida positiva} si existe un $\alpha \in \mathbb{R}^+$ tal que
	\begin{equation}
		x^TAx \geq \alpha x^T x, \quad \text{para todo } x \in \mathbb{R}^n.
	\end{equation}
	Esta es \textit{semidefinida postiva} si
	\begin{equation}
		x^TAx \geq 0, \quad \text{para todo } x \in \mathbb{R}^n.
	\end{equation}
\end{definition}

\begin{definition}
	Una matriz $n \times n$ cuadrada A se dice \textit{no singular} si para cada $b \in \mathbb{R}^n$, existe $x\in\mathbb{R}^n$ tal que $Ax=b$.
\end{definition}

\begin{definition}
	Una matriz cuadrada $Q$ se dice \textit{ortogonal} si cumple $QQ^T=Q^TQ=I$
\end{definition}

\begin{definition}
	Si tomamos los sistemas de vectores de una una matriz $A_{m \times n}$, $\sucesion{u_i}_{i=1}^n$ y $\sucesion{v_i}_{i=1}^m$, al número máximo de vectores linealmente independientes se le llama \textit{rango}, tanto de los sistemas de vectores como de la matriz $A$. Si $n<m$, se dice que $A$ es de \textit{rango completo} si su rango es $n$, que es el máximo posible.
\end{definition}

Lo más común en este caso para resolver numéricamente es usar distintos tipos de factorización sobre la matriz $J^TJ$ o sobre $J$, para luego resolver con sustituciones triangulares.
El primer algoritmo que se plantea es a partir de la \textbf{factorización de Cholesky}, comenzando por computar la matriz de coeficientes $J^TJ$ y el lado derecho $J^Ty$. Después se computa la factorización de Cholesky
\begin{equation}
	J^TJ = \bar R^T\bar R.
\end{equation}
Para que esta exista, necesitamos que $m \geq n$ y que $J$ sea de rango $n$,
lo que permite que $J^TJ$ sea simétrica y definida positiva.
Se termina realizando las dos sustituciones triangulares con los factores de Cholesky para encontrar $x^*$.
La principal desventaja de este método es que el condicionamiento de $J^TJ$ es el cuadrado del condicionamiento de $J$, y esto puede llevar a errores de aproximación.
Además, si $J$ está mal condicionada, ni si quiera se puede llevar a cabo la factorización.

Una segunda posibilidad es basarse en la \textbf{factorización QR}, que evita el problema de depender del cuadrado del condicionamiento de $J$, ya que aplicaremos la factorización directamente a $J$. Se aprovecha que la norma euclídea no se ve afectada por trasformaciones ortogonales para partir de la igualdad
\begin{equation}
	\Vert Jx- y \Vert = \Vert Q^T(Jx-y) \Vert, 
\end{equation}
siendo $Q$ una matriz ortogonal $m \times m$. Factorizando con una matriz pivote $\Pi$, la solución es
\begin{equation}
	x^* = \Pi R^{-1}Q_1^Ty.
\end{equation}
Donde $R$ es una matriz $n \times n$ triangular superior con elementos positivos en la diagonal y $Q_1$ son las primeras $n$ columnas de $Q$, ambos producto de la factorización QR.

En este caso, el error relativo es proporcional al condicionamiento de $J$ y no de su cuadrado. Aún así, hay situaciones en las que necesitamos asegurar que la obtención sea de algún modo más robusta o en las que queremos más información acerca de la sensibilidad de la solución a perturbaciones en $J$ o en $y$. Esto es, queremos asegurarnos que pequeños cambios en $J$ o $y$ no afecten significativamente a la solución obtenida, lo cual pondría en duda nuestro método ya que estas perturbaciones se pueden dar durante la computación.

Nos basaremos ahora en la \textbf{descomposición de valores singulares (DVS)}, cuya resolución una vez realizada la descomposición se enfoca de forma similar a la anterior. Primero se realiza el algoritmo (DVS) para obtener $J = USV^T$, con $U$ matriz $m \times m$, $V$ matriz $n \times n$, ambas ortogonales y $S$ matriz $n \times n$ de elementos diagonales $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n > 0$. Aprovechamos estas propiedades para obtener
\begin{equation}
	x^* = VS^{-1}U_1^Ty.
\end{equation}
Denotando por $u_i \in \mathbb{R}^m$ y $v_i \in \mathbb{R}^n$ las columnas de $U$ y $V$ respectivamente, escribimos
\begin{equation}
	x^* = \sum_{i=1}^n \frac{i_i^Ty}{\sigma_i}v_i.
\end{equation}
Fórmula de donde obtenemos información útil como la sensibilidad al aproximar $x^*$.

Las 3 opciones son buenas según las condiciones en las que nos encontremos. La resolución basada en Cholesky es útil cuando $m\gg n$ y es práctico trabajar almacenando $J^TJ$ en lugar de $J$, siempre y cuando $J$ sea de rango completo y bien condicionada. Si esto último no se cumple, la factorización QR es un enfoque más equilibrado, mientras que DVS es el más costoso a cambio de ser el más fiable.

Por último, mencionar que existen métodos para problemas muy grandes, en los que se usan técnicas iterativas como el método de gradientes conjugados para resolver el sistema.

\section{El método de Gauss-Newton}

Comenzamos los métodos de minimización del problema no lineal (\ref{eq:lsp}) con el método de Gauss-Newton. La forma más sencilla de abordar el término de segundo orden $S(x)$ de $G_k$ en el modelo cuadrático (\ref{eq:q}) es obviarlo. Así, resulta
\begin{equation}
\begin{split}
\bar q^{(k)}(x) =& \frac{1}{2}r(x_k)^Tr(x_k)+(J(x_k)^Tr(x_k))^T(x-x_k)+ \\
	& +\frac{1}{2}(x-x_k)^T(J(x_k)^TJ(x_k))(x-x_k),
\end{split}
\end{equation}
y por tanto,
\begin{equation}
	x_{k+1} = x_k + p_k = x_k -(J(x_k)^TJ(x_k))^{-1}J(x_k)^Tr(x_k).
\end{equation}

Notar que para que esté bien definido, la matriz Jacobiana $J(x)$ tiene que ser de rango completo. Gracias a la aproximación $\nabla^2 f(x_k) \approx J(x_k)^TJ(x_k)$, hacemos que la única dificultad del algoritmo sea resolver un sistema lineal, ya que evitamos computar $\nabla^2 r_j,\,j=1,2,\ldots ,m$. En algunas situaciones, cuando nos vamos acercando a la solución $x^*$, esta aproximación suele ser más precisa, ya sea porque los residuos $r_i$ o $\norm{\nabla^2 r_i}$ es cercano a cero. La eficacia del método dependerá por tanto de lo buena que sea esta aproximación.

\begin{algorithm}[Método de Gauss-Newton] \leavevmode
\label{al:g-n}
\begin{steps}
	\item $x_0$ y $\epsilon > 0$ dados, $k:=0$
	\item Si $\norm{g_k} \leq \epsilon$, STOP.   
	\item Obtener el paso $p_k$ resolviendo
		\begin{equation}
			J(x_k)^TJ(x_k)p_k = -J(x_k)^Tr(x_k)
		\end{equation}
	\item Definimos $x_{k+1} = x_k + p_k$ y actualizamos $k=k+1$. Ir a Paso 2. \quad \qedsymbol
\end{steps}
\end{algorithm}

Siempre y cuando $J$ tenga rango completo y el gradiente $\nabla f_k = J(x_k)^Tr(x_k)$ sea no nulo, la dirección $p_k$ es una dirección descendente.
Como vemos en el \textit{Paso 3} resolvemos un caso análogo al problema lineal. Debido a esto, $p_k$ es también la solución de
\begin{equation}
	\min_{p_k} \frac{1}{2} \norm{J(x_k)p_k + r_k}^2,
\end{equation}
y por eso decimos que el método de Gauss-Newton es en realidad una linealización del problema no lineal de mínimos cuadrados y, como el método de Newton, resultará localmente convergente de manera cuadrática bajo las condiciones de este.
Por tanto, el error de aproximación de la solución dependerá también de como resolvamos el problema lineal, y aplican los casos vistos en el apartado anterior.

\begin{theorem}
	\label{th:gn1}
	Sea $f: \mathbb{R}^n \rightarrow \mathbb{R}$ y $f \in \mathcal{C}^2$. Supongamos que $x^*$ es mínimo local del problema (\ref{eq:lsp}), $J(x^*)^TJ(x^*)$ es definida positiva y la sucesión $\sucesionxk$ generada por el algoritmo (\ref{al:g-n}) converge a $x^*$. Si $G(x)$ y $(J(x)^TJ(x))^{-1}$ son lipschitzianas en una vecinidad de $x^*$, entonces
\begin{equation}
	\norm{x_{k+1}-x^*} \leq \Vert (J(x^*)^TJ(x^*))^{-1} \Vert \norm{S(x^*)} \norm{x_k-x^*}
	+ O(\norm{x_k-x^*}).
\end{equation}
\end{theorem}

Este teorema nos dice esencialmente que la convergencia del método depende de $S(x^*)$. Cuando $S(x^*)=0$ este converge de forma cuadrática y, según aumente, la convergencia es menor.

\begin{theorem}
	\label{th:gn2}
	Sea $f:D \subset \mathbb{R}^n \rightarrow \mathbb{R}$ y $f \in \mathcal{C}^2(D)$, con $D$ conjunto abierto convexo. Sea $J(x)$ lipschitziana en $D$ y $\norm{J(x)} \leq \alpha, \, \forall x \in D$. Supongamos que existe $x^* \in D$ y $\lambda, \sigma \geq 0$ tal que $J(x^*)^Tr(x^*)=0$,
$\lambda$ es el autovalor más pequeño de $J(x^*)^TJ(x^*)$, y
	\begin{equation}
		\norm{(J(x)-J(x^*))^Tr(x^*)} \leq \sigma \norm{x-x_k},\,\forall x \in D.
	\end{equation}
	Si $\sigma < \lambda$, para cualquier $c\in (1,\lambda / \sigma)$, existe $\epsilon > 0$ tal que para todo $x_0 \in N(x^*, \epsilon)$, la sucesión resultante $\sucesionxk$ del algoritmo \ref{al:g-n} está bien definida, converge a $x^*$ y satisface
	\begin{equation}
	\norm{x_{k+1}-x^*} \leq \frac{c\sigma}{\lambda} \norm{x_k-x^*}+\frac{c\alpha\sigma}{2\lambda}\norm{x_k-x^*}^2
	\end{equation}
	y
	\begin{equation}
	\norm{x_{k+1}-x^*} \leq \frac{c\alpha+\lambda}{2\lambda}\norm{x_k-x^*} < \norm{x_k-x^*}.
	\end{equation}
\end{theorem}

\begin{theorem}
	Manteniendo las suposiciones de los dos teoremas \ref{th:gn1} y \ref{th:gn2}, si $r(x^*)=0$, entonces existe $\epsilon>0$ tal que para cualquier $x_0 \in N(x^*,\epsilon)$, la sucesión $\sucesionxk$ obtenida del método de Gauss-Newton converge a $x^*$ con orden cuadrático.
\end{theorem}

Para concluir observamos que el método encaja dentro de los métodos de búsqueda de línea, tomando $d_k=p_k$ en  (\ref{eq:linesearch}) y calculando una longitud de paso $\alpha_k$,
\begin{equation}
	x_{k+1} = x_k-\alpha_k(J(x_k)^TJ(x_k))^{-1}J(x_k)^Tr(x_k).
\end{equation}

\chapter{El método de Levenberg-Marquardt}

El método de Levenberg-Marquardt soluciona la necesidad de que $J$ sea de rano completo cambiando el enfoque de búsqueda de línea por el de región de confianza.
Lo hace manteniendo la raíz del método de Gauss-Newton, la linealización del problema obviando el término cuadrático, esto es, usando la aproximación $\nabla^2 f(x_k) \approx J(x_k)^TJ(x_k)$.
Este cambio de enfoque surge de que la linealización pierde efectividad según nos alejamos de $x_k$, por lo que conviene restringir el tamaño de $p=(x-x_k)$. Consideramos el problema:
\begin{equation}
\label{min:sublev}
	\min_{p} \quad \frac{1}{2} \norm{J_kp+r_k}^2,\quad \text{s.a.} \norm{p} \leq \Delta_k,
\end{equation}
Donde $\Delta_k>0$ es el radio de la región de confianza.
De hecho, se cree que esta solución característica de los métodos de región de confianza nace con este método en concreto. Otra forma de escribir el modelo es
\begin{equation}
	m_k(p)=\frac{1}{2}\norm{r_k}^2 + p^TJ_k^Tr_k + \frac{1}{2}p^TJ_k^TJ_kp,
	\quad \text{s.a.} \norm{p} \leq \Delta_k.
\end{equation}
La solución de este subproblema queda caracterizada por el sistema
\begin{equation}
	\label{eq:lmlp}
	(J^TJ+\lambda I)p=-J^Tr.
\end{equation}
%El siguiente lema nos da otra caracterización del problema.
\begin{lemma}
El vector $p$ es solución del subproblema (\ref{min:sublev}) si y solo si $p$ es factible y existe un $\lambda \geq 0$ tal que
\begin{align}
	(J^TJ+\lambda I)p&=-J^Tr, \\
	\lambda (\Delta-\norm{p})&=0.
\end{align}
\end{lemma}
\begin{proof}
Es consecuencia del teorema \ref{th:trustregion}. Solo hay que ver que se cumplen las 3 condiciones, las dos primeras se siguen del propio lema. La tercera pide que $(J^TJ+\lambda I)$ sea semidefinida positiva, y lo es por serlo $J^TJ$ y por ser $\lambda>0$
\end{proof}

En concreto, como $(J^TJ+\lambda I)$ es definida positiva, la solución de (\ref{eq:lmlp}) es una dirección descendente. Lo que nos quiere decir este lema es que si la solución obtenida por el método de Gauss-Newton cae estrictamente dentro de la región de confianza, esta solucionará también el subproblema (\ref{min:sublev}).
En otro caso, existe un $\lambda>0$ que permite encontrar una solución a
\begin{equation}
\label{eq:lmsol}
	(J^TJ+\lambda I)p = -J^Tr,
\end{equation}
con $\norm{p}=\Delta$. Si $\lambda=0$, la solución del problema es la de Gauss-Newton, y según aumenta $\lambda$, la solución se acerca a la del método de máximo descenso.
Veamos una serie de propiedades del método de Levenberg-Marquardt y de $p$ en función de $\lambda$. 

\begin{theorem}
	Si $\lambda$ aumenta desde cero monótonamente, $\norm{p(\lambda)}$ decrece de forma estrictamente monótona.
\end{theorem}
\begin{proof}
	Por un lado,
	\begin{equation}
		\label{eq:dem1}
		\frac{\mathrm{d}}{\mathrm{d}\lambda}\norm{p}=
		\frac{\mathrm{d}}{\mathrm{d}\lambda}(p^Tp)^{\frac{1}{2}}+
		\frac{p^T\frac{\mathrm{d}p}{\mathrm{d}\lambda}}{\norm{p}}.
	\end{equation}
	Derivando (\ref{eq:lmsol}) respecto a $\lambda$, obtenemos
	\begin{equation}
		\label{eq:dem2}
		p + (J^TJ+\lambda I)\frac{\mathrm{d}p}{\mathrm{d}\lambda} = 0,
	\end{equation}
	de esta y (\ref{eq:lmsol}) resulta
	\begin{equation}
		\label{eq:dem3}
		\frac{\mathrm{d}p}{\mathrm{d}\lambda}=(J^TJ+\lambda I)^{-2}g,
	\end{equation}
	con $g=J^Tr$. Sustituyendo en (\ref{eq:dem1}) y usando (\ref{eq:lmsol}), nos queda
	\begin{equation}
		\frac{\mathrm{d}}{\mathrm{d}\lambda}\norm{p}=
		-\frac{g^T(J^TJ+\lambda I)^{-3}g}{\norm{p}}.
	\end{equation}
	Cuando $\lambda \geq 0,\, J^TJ+\lambda I$ es definida positiva. Por tanto $\norm{p(\lambda)}$ de forma estrictamente monótona.
\end{proof}

\begin{theorem}
	Sea $\lambda_k>0$, si $p_k$ es solución de (\ref{eq:lmlp}), entonces $p_k$ es solución global de el subproblema
\begin{equation}
\min_{p} \quad \frac{1}{2} \norm{J_kp+r_k}^2,\quad \text{s.a.} \norm{p} \leq \norm{p_k},
\end{equation}
\end{theorem}
\begin{proof}
	.
\end{proof}



%%%%%%%%     Póñense a bibliografía, Apéndices e o índice alfabético     %%%%%%%%
\appendix
\renewcommand{\thechapter}{\Roman{chapter}}
\chapter{Título del Anexo I}


\chapter{Título del Anexo II}

\backmatter

\begin{thebibliography}{99}

%%%%%%%%----Exemplo de entradas bibliográficas:
%


\bibitem{Noc} Nocedal, J., \& Wright, S. (2006). \emph{Numerical Optimization} (2nd ed.). Springer.

\bibitem{Sun} Sun, W., \& Yuan, Y.-X. (2006). \emph{Optimization theory and methods: Nonlinear programming} (2006th ed.). Springer.

%
\end{thebibliography}




\end{document}


