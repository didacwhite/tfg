%%%%%%%%%%     PREÁMBULO      %%%%%%%%%%%%%%%

%%%%%%%%%   Non modificar este preámbulo, salvo que se sepa ben o que se está a facer   %%%%%%%%%

%%%%%%  Traballando nun Mac, se usas TeXShop, en  Preferencias -> Código -> codificación de  caracteres (UTF-8)  %%%%

\documentclass[11pt,a4paper]{book}

% Formato do documento
\usepackage[papersize={210mm,297mm},lmargin=2.5cm,rmargin=2.5cm,top=3.5cm,bottom=3.5cm]{geometry}   % Marxes
\renewcommand{\baselinestretch}{1.25}                                                               % Fixamos o interliñado
\setlength{\parskip}{8pt}                                                                           % Separación entre parágrafos


\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{latexsym,pdfsync,xcolor,graphicx}
\usepackage[nottoc]{tocbibind}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}					
\usepackage[spanish]{babel}			
\usepackage{tikz}
\usepackage{appendix}
\usepackage{hyperref}








% Cabeceira
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{\textbf{\thechapter. #1}}{}} % Formato para o capítulo: N. Nome
\renewcommand{\sectionmark}[1]{\markright{\textbf{\thesection. #1}}}  % Formato para a sección: N.M. Nome

\fancyhead[LO]{\rightmark}           % Nas páxinas impares, parte esquerda do encabezado, aparecerá o nome do capítulo
\fancyhead[RE]{\leftmark}            % Nas páxinas pares, parte dereita do encabezado, aparecerá o nombre da sección
\fancyhead[RO,LE]{\textbf{\thepage}} % Números de páxina nas esquinas dos encabezados




%%%   \pagestyle{plain}   Cando os títulos dos capítulos son moi longos, a opción por defecto das cabeceiras pode ser inapropiada;
%%%  nestes e noutros casos semellantes, pódese usar a opción "plain", que suprime as cabeceiras, deixando soamente a numeración das páxinas.


%%%%%%%      NON ESCRIBIR AQUÍ  (COMANDO PORTADA) %%%%%%%%%%%

\newcommand{\portada}[3]{
\thispagestyle{empty}

\definecolor{azulUSC}{RGB}{13,38,120}

\includegraphics[width=13cm]{logomath.pdf}

\vspace*{2cm}

\centerline{{\Large\bf  Traballo Fin de Grao}}

\vspace{2.6cm}

{\center{\Huge\bfseries\color{azulUSC} #1}\par}		

\vspace{1cm}

{\Large

\centerline{#2}		

\vspace{6.4cm}

\centerline{\sf #3}        

\vspace{1.25cm}

\centerline{\sf UNIVERSIDADE DE SANTIAGO DE COMPOSTELA}
}

\enlargethispage{1cm}
\clearpage
}

%%%%%%%      NON ESCRIBIR AQUÍ  (COMANDO PRIMEIRA PÁXINA) %%%%%%%%%%%

\newcommand{\primeira}[3]{
\thispagestyle{empty}
\enlargethispage{1cm}
\begin{center}\Large

{\sf GRAO DE MATEM\'ATICAS}

\bigskip
{\bfseries Traballo Fin de Grao}
\vspace{4cm}

{\bfseries\Huge #1} 		
\vspace{1.2cm}

{#2}		\par		     
\end{center}

\vspace{6.5cm}

\begin{center}
\Large
{#3}	\par			    

\vspace{1cm}
{\sf UNIVERSIDADE DE SANTIAGO DE COMPOSTELA}
\end{center}
\clearpage
}

%%%%%%%    Teoremas   %%%%%%%%%

\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{corollary}[theorem]{Corolario}
\newtheorem{algorithm}[theorem]{Algoritmo}
\newtheorem{axiom}[theorem]{Axioma}
\newtheorem{case}[theorem]{Caso}
\newtheorem{conclusion}[theorem]{Conclusión}
\newtheorem{condition}[theorem]{Condición}
\newtheorem{conjetura}[theorem]{Conjetura}				
\newtheorem{criterion}[theorem]{Criterio}
\newtheorem{ejercicio}[theorem]{Ejercicio}	
			
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definición}
\newtheorem{ejemplo}[theorem]{Ejemplo}				
\newtheorem{agradecimientos}[theorem]{Agradecimientos}	

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Observación}
\newtheorem{notation}[theorem]{Notación}
\newtheorem{problem}[theorem]{Problema}
\newtheorem{solution}[theorem]{Solución}

\addto\captionsspanish{
	\renewcommand{\contentsname}{\'Indice} 
	\renewcommand\appendixname{Anexo}
	\renewcommand\appendixpagename{Anexos}
}
%%%%%%%%%%%%%%%%%%   Fin do Preámbulo   %%%%%%%%%%%%%%%%




\begin{document}
\frontmatter

%%%%%%%%%%%      PORTADA      %%%%%%%%%%%%

\portada
{Resolución numérica del problema no lineal de mínimos cuadrados.\vspace{5pt}
Aplicaciones a la estimación de parámetros de modelos matemáticos.}		%  escribir aquí o  Título
{Dídac Blanco Morros}		%  escribir aquí o/a  Autor/a
{Curso Acad\'emico}		  	%  escribir aquí a Data de presentación, na forma:  mes, ano

%%%%%%%%     PRIMEIRA PÁXINA    %%%%%%%%%%%%%%%%%%

\mbox{}
\thispagestyle{empty}
\clearpage

\setcounter{page}{1}

\primeira
{Resolución numérica del problema no lineal de mínimos cuadrados. 
Aplicaciones a la estimación de parámetros de modelos matemáticos.}		%  escribir aquí o  Título
{Dídac Blanco Morros}		%  escribir aquí o/a  Autor/a
{Febrero, 2022}		  	%  escribir aquí a Data de presentación, na forma:  mes, ano

\thispagestyle{empty}

%%%%%%%%%%%      PROPOSTA DE TRABALLO      %%%%%%%%%%%%%%%%

\vspace*{.5 cm}

\chapter*{Trabajo propuesto}

\vspace{1 cm}

\begingroup
\renewcommand*{\arraystretch}{2.2}
\begin{tabular}{|l|}
	\hline
	
	{\bf \'Area de Co\~necemento:  \/ Matemática Aplicada}\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}
			\bf T\'{\i}tulo:   \/ \bf Resolución numérica do problema non linear de mínimos cadrados. Aplicacións á estimación de parámetros de modelos matemáticos.
			\vspace{.2cm}}
	\end{minipage}\\ \hline
	\bf Breve descrici\'on do contido\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}
O problema non linear de mínimos cadrados zurde en moutas aplicacións da ciencia e da enxeñería: no axuste dun conxunto de datos a un modelo matemático, na estimación de parámetros, na aproximación de funcións, etc. O obxectivo do traballo fin de grao é o estudo de métodos numéricos para abordar o problema de minimización resultante. centrándose especialmente no algoritmo de Levenberg-Marquardt. O estudante estudará o método, no marco dos métodos de optimización con rexión de confianza e familiarizarase co seu uso mediante o comando Isqnonlin de Matlab. As metodoloxías estudadas aplicaranse a exemplos académicos e á estimación de parámetros de distintos modelos matemáticos a partir de datos experimentais.
		
		\vspace{.2cm}}
	\end{minipage}\\ \hline
	{\bf Recomendaci\'ons}\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}


		\vspace{.2cm}}
	\end{minipage}\\ \hline
	{\bf Outras observaci\'ons}\\ \hline
	\begin{minipage}{11.5cm}
		{\vspace*{.2cm}


			\vspace{.2cm}}
	\end{minipage}\\ \hline
\end{tabular}
\endgroup

\clearpage

\thispagestyle{empty}

\tableofcontents

\clearpage

\thispagestyle{empty}

\mbox{}

\clearpage

%%%%%%%%%     RESUMO---RESUMEN---ABSTRACT     %%%%%%%%%%%%%

\phantomsection
\addcontentsline{toc}{chapter}{Resumen}		
\chapter*{}

\section*{Resumen}

% escribir aquí resumo en español




\vspace{1.5cm}

\section*{Abstract}



% escribir aquí resumo en inglés



\clearpage

\thispagestyle{empty}

%%%%%%%%%    INTRODUCIÓN     %%%%%%%%%%%%%%%%%%
%%%%%%%%%    De non incluír unha INTRODUCIÓN, suprímese    %%%%%%%%%%

\phantomsection
\chapter*{Introducci\'on}\addcontentsline{toc}{chapter}{Introducci\'on}

\markboth{INTRODUCCI\'ON}{INTRODUCCI\'ON}

%%%%%%%%%%%     fin da INTRODUCIÓN     %%%%%%%%%%%%%

\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				S
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				 T
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				  A
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%				   R
%%%%%%%%     Póñense os Capítulos    %%%%%%%%%%%%%%%				    T



\chapter{Fundamentos de la optimización sin restricciones}
% El problema de los mínimos cuadrados es un caso particular de optimización sin restricciones, y es por ello que comenzaremos introduciendo sus fundamentos. Ya que el problema de mínimos cuadrados es usado en multitud de campos para estimar parámetros, este es de los más utilizados dentro de los problemas de optimización sin restricciones.

Un problema de optimización sin restricciones tiene la forma 
\begin{equation}
	\min_{x}f\left(x\right),
	\label{eq:minf}
\end{equation}
donde $x\in\mathbb{R}^{n}$ y $f : \mathbb{R}^{n} \rightarrow \mathbb{R}$ es continuamente diferenciable, la llamamos \textbf{función objetivo}. 
Notar que podemos usar esta formulación para referirnos tanto a los problemas de minimización como de maximización, basta sustituir $f$ por $-f$. 

La dificultad de un problema como este viene de no conocer el comportamiento global de $f$, normalmente solo disponemos de la evaluación de f en algunos puntos, y a lo mejor de algunas de sus derivadas. El trabajo de los algoritmos de optimización es identificar la solución sin usar demasiado tiempo ni almacenamiento computacional.

% W. Sun es más formal en este caso para definir ambos mínimos 
\begin{definition}
	Un punto $x^*$ se dice \textit{mínimo local} si existe $\delta > 0$ tal que $f(x^*) \leq f(x)$ para todo $x \in \mathbb{R}^n$ que satisface $\Vert x - x^* \Vert < \delta$.
	Un punto $x^*$ se dice \textit{mínimo local estricto} si existe $\delta > 0$ tal que $f(x^*) < f(x)$ para todo $x \in \mathbb{R}^n$ que satisface $\Vert x - x^* \Vert < \delta$ con $x \neq x^*$.
\end{definition}

\begin{definition}
	Un punto $x^*$ se dice \textit{mínimo global} si $f(x^*) \leq f(x)$ para todo $x \in \mathbb{R}^n$.
	Un punto $x^*$ se dice \textit{mínimo global estricto} si $f(x^*) < f(x)$ para todo $x \in \mathbb{R}^n$ con $x \neq x^*$.
\end{definition}

Como no se suele tener un conocimiento a gran escala de $f$ debido a su coste, la mayoría de algoritmos solo encuentran mínimos locales, lo cual es suficiente para muchos casos prácticos.

Aún así, los algoritmos para encontrar mínimos globales se suelen construír a partir de una secuencia de otros algoritmos de optimización local. También podemos aprovechar características fáciles de detectar en la función objetivo, como la convexidad, que nos asegura que un mínimo local será también global.

%% REPASO DE ALGORITMOS

%% \section{Generalidades de los algoritmos}

%% histórico??
% referencia 	NOCEDAL 2.2

\begin{definition}
	Sea $f: \mathbb{R}^n \rightarrow \mathbb{R}$ diferenciable en
$x \in \mathbb{R}^n$ tal que $\langle \nabla f(x), d \rangle < 0$, entonces a $d$ se le llama \textit{dirección descendente} de $f$ en $x$.
\end{definition}

\begin{theorem}[Teorema de Taylor] %% COMPATIBILIZAR CON RESULTADO ANTERIOR
	
	Sea $f: \mathbb{R}^n \rightarrow \mathbb{R}$ continuamente diferenciable y sea $p \in \mathbb{R}^n$, tenemos que 
	\begin{equation}
		f(x+p) = f(x) + \nabla f(x+tp)^Tp, \quad t\in (0,1).
	\end{equation}
	Si además, $f$ es dos veces continuamente diferenciable
	\begin{equation}
		\nabla f(x+p) = \nabla f(x)
		+ \int_0^1 \nabla^2 f(x+tp)p\:dt,
	\end{equation}
	y
	\begin{equation}
		f(x+p) = f(x) + \nabla f(x)^Tp
		+ \frac{1}{2}p^T \nabla^2 f(x+tp)p, \; t\in (0,1).
		\label{eq:thty}
	\end{equation}
\end{theorem}


\begin{proposition}
Partiendo de la reformulación de (\ref{eq:thty}) y teniendo en cuenta que el último término es el error de aproximación $o(t)$
\begin{equation}
	f(x_k + td) = f(x_k) + t \nabla f(x_k)^Td + o(t),
\end{equation}
se cumple que
\begin{equation}
	\exists \delta > 0 \text{ tal que } f(x_k + td) < f(x_k)
	\quad \forall t \in (0, \delta)
\end{equation}
si y solo si $d$ es una dirección descendiente de $f$ en $x_k$.

\end{proposition}

Tratemos ahora las condiciones de optimalidad.

\begin{theorem}[Condición Necesaria de Primer Orden]
	Sea $f:D\subset \mathbb{R}^n \rightarrow \mathbb{R}$ continuamente diferenciable en un conjunto abierto $D$. Si $x^*$ es un mínimo local de (\ref{eq:minf}), entonces $\nabla f(x^*) = 0$.
\end{theorem}

\begin{theorem}
	(Condición Necesaria de Segundo Orden)
	Sea $f:D\subset \mathbb{R}^n \rightarrow \mathbb{R}$ dos veces continuamente diferenciable en un conjunto abierto $D$. Si $x^*$ es un mínimo local de (\ref{eq:minf}), entonces $\nabla f(x^*) = 0$ y $\nabla^2 f(x^*)$ es definida positiva.
\end{theorem}

\begin{theorem}[Condición Suficiente de Segundo Orden]
	Sea $f:D\subset \mathbb{R}^n \rightarrow \mathbb{R}$ dos veces continuamente diferenciable en un conjunto abierto $D$. Si $\nabla f(x^*) = 0$ y $\nabla^2 f(x^*)$ es definida positiva, entonces $x^* \in D$ es un mínimo local.
\end{theorem}

Para poder dar un último resultado para soluciones óptimas en minimización, veamos unas últimas definiciones.

\begin{definition}
	Sea $S\subset \mathbb{R}^n$ y sean $x_1, x_2 \in S$ cualesquiera. Si
	$\alpha x_1 + (1 - \alpha)x_2 \in S$ para todo $\alpha \in [0,1]$, entonces se dice que D es un \textit{conjunto convexo}.
\end{definition}
\begin{definition}
	Sea $S \subset \mathbb{R}^n$ un conjunto convexo no vacío. Sea $f: S \subset \mathbb{R}^n \rightarrow R$. Si para cualquiera $x_1, x_2 \in S$ y
	$\alpha \in (0,1)$, se cumple que
	\begin{equation}
		f(\alpha x_1 + (1-\alpha)x_2) \leq \alpha f(x_1) + (1-\alpha)f(x_2),
	\end{equation}
	se dice que $f$ es una función convexa en $S$.
\end{definition}

\begin{theorem}
	Sea $S \subset \mathbb{R}^n$ un conjunto convexo no vacío y
	$f:S \subset \mathbb{R}^n \rightarrow \mathbb{R}$ una función convexa. Si $x^*$ es mínimo local de (\ref{eq:minf}), entonces también es mínimo global.
\end{theorem}

\begin{theorem} \label{th:convx}
	Sea $f: \mathbb{R}^n \rightarrow \mathbb{R}$ convexa y diferenciable, entonces $x^*$ es un mínimo global si y solo si $\nabla f(x^*) = 0$.
\end{theorem}

Todo algoritmo de optimización sin restricciones comienza con un punto de partida, denotado normalmente como $x_{0}$. Aunque generalmente el usuario introduce una estimación razonable, el punto puede ser elegido por el algoritmo, tanto de forma sistemática como aleatoria. El algoritmo itera sobre $x_{0}$, creando una sucesión $\left\{x_k\right\}_{k=0}^n$ la cual termina cuando no pueda continuar o cuando ya se haya acercado razonablemente a la solución. Para decidir como se avanza de un $x_k$ al siguiente, los algoritmos utilizan información sobre $f(x_k)$ o incluso en los puntos anteriores $x_0,\, x_1,\, \ldots,\,x_{k-1}$ con el objetivo de que $f(x_{k+1})<f(x_{k})$. Hablaremos de las dos estrategias fundamentales que se utilizan para avanzar de $x_k$ a $x_{k+1}$, \textit{búsqueda de línea} y \textit{región de confianza}.

\section{Búsqueda de línea}

En este caso el algoritmo tiene dos tareas a partir de cada iteración, primero elige una \textit{dirección} $d_k$ y tomando el punto de partida busca en esa dirección el nuevo valor. Es decir, dado $x_k$
\begin{equation}
	x_{k+1} = x_k + \alpha_kd_k
\end{equation}
para un $d_k$ elegido previamente, y un \textit{paso} $\alpha_k$ obtenido solucionando otro problema de minimización más simple por ser unidimensional:
\begin{equation}
	\min_{\alpha_k>0}f\left(x_k+\alpha_kd_k\right).
	\label{min:alphak}
\end{equation}

Si se toma el $\alpha_k$ óptimo se le llama búsqueda de línea \textit{exacta} u \textit{óptima}. Para evitar el gran coste computacional que puede llegar a tomar, lo más común es tomar un $\alpha_k$ que aporte un descenso aceptable, en cuyo caso se le llama búsqueda de línea \textit{inexacta} o \textit{aproximada}. Desde el nuevo punto se busca otra dirección y paso para repetir el proceso.
Veamos brevemente cómo se eligen $d_k$ y $\alpha_k$.

La mayor parte de algoritmos de este tipo necesitan que $d_k$ sea una dirección descendente, esto es, $d_k^T \nabla f_k < 0$,
lo cual asegura que en esa dirección se podrá reducir el valor de $f$. Esta suele tener la forma
\begin{equation}
	d_k = -B_k^{-1} \nabla f_k
\end{equation}
con $B_k$ una aproximación de la matriz Hessiana $\nabla^2 f(x_k)$ simétrica y no singular. Según lo que acabamos de decir, necesitamos que $B_k$ sea definida positiva. En las tres corrientes principales se elige un $B_k$ distinto, en el \textit{método del descenso máximo} o \textit{descenso del gradiente}, se usa la matriz identidad $I$.
En el \textit{método de Newton} se usa la matriz exacta, mientras que en los \textit{métodos Quasi-Newton} la matriz Hessiana es aproximada para cada $x_k$.

En el caso de la elección de $\alpha_k$, el caso ideal sería encontrar el óptimo en \ref{min:alphak}, pero esto es en general demasiado costoso.
Debido a ese coste, se suelen utilizar búsquedas inexactas probando una serie de puntos hasta que alguno cumpla unas condiciones preestablecidas con las que se acepta el paso dado.
Estas condiciones son por ejemplo las condiciones \textit{Wolfe} o las condiciones \textit{Goldstein}.
Esta elección se hace en dos fases, primero un proceso elige un intervalo conteniendo los pasos deseables y una segunda fase donde se va reduciendo el intervalo por técnicas de interpolación o bisección. % el mejor?

\subsection{Método de Newton}
Veamos brevemente las ideas detrás del método de Newton, influyentes en los demás planteamientos. La idea principal es usar la aproximación cuadrática $q^{(k)}$ de la función objetivo,
\begin{equation}
	q^{(k)}(p) = f(x_k)+\nabla f(x_k)^Tp + \frac{1}{2}p^T \nabla^2 f(x_k)p,
	\label{eq:NewtonQ}
\end{equation}
si $f:\mathbb{R}^n \rightarrow \mathbb{R}$ dos veces continuamente diferenciable, $x_k \in \mathbb{R}^n$ y la Hessiana $\nabla^2f(x_k)$ es definida positiva. En tal caso aproximamos $f(x_k + p) \approx q^{(k)}(p)$.

Minimizando $q^{(k)}(p)$ obtenemos la fórmula de Newton, denotando $G_k=\nabla^2f(x_k)$ y $g_k = \nabla f(x_k)$:
\begin{equation}
	x_{k+1} = x_k - G_k^{-1} g_k.
	\label{eq:NewtonIter}
\end{equation}

\begin{theorem}[Teorema de Convergencia del Método de Newton]
	Sea
	$f \in \mathcal{C}^2$ y $x_k$ lo suficientemente cerca a
	la solución $x^*$ del problema de minimización con
	$g(x^*)=0$. Si la Hessiana $G(x^*)$ es definida positiva y
	$G(x)$ satisface la condición de Lipschitz
	\begin{equation}
		\vert G_{ij}(x) - G_{ij}(y) \vert \leq 
		\beta \Vert x-y \Vert,
		\text{ para algún } \beta,
		\text{ y para todo } i,j,
	\end{equation}
	siendo $G_{ij}(x)$ el elemento en la posición $(i,j)$ de la matriz
	$G(x)$, entonces para todo $k$, la iteración (\ref{eq:NewtonIter})
	está bien definida y la sucesión $\left\{ x_k \right\}$
	generada converge a $x^*$ de forma cuadrática.
\end{theorem}

\section{Región de confianza}

Esta estrategia enfoca el problema de otro modo, primero se fija una distancia máxima $\Delta_k$ para definir una región, generalmente de la forma
\begin{equation}
	\Omega_k = \left\{x : \Vert x-x_k \Vert \leq \Delta_k \right\}
\end{equation}
y luego ya se busca la dirección y paso.
A partir de la información conocida de $f$, para cada $x_k$ se modela una función $m_k$ que se comporte de manera similar a $f$ cerca de este punto.
Se suele utilizar el modelo cuadrático $q^{(k)}$ visto anteriormente (\ref{eq:NewtonQ}), aprovechando la notación usada en el apartado anterior:
\begin{equation}
	m_k := q^{(k)}(p) = f(x_k) + g^T_kp + \frac{1}{2}p^TG_kp.
\end{equation}

Como hemos visto, este modelo se utiliza en los métodos de búsqueda de línea para determinar la dirección de búsqueda, mientras que en este caso lo usamos para tener una representación adecuada de la función objetivo y así elegir el mínimo dentro de esta región.
Este método nos evita el problema de que la Hessiana no sea definida positiva.
En cada iteración, una vez elegido $\Delta_k$ se resuelve el siguiente problema:
\begin{equation}
\begin{aligned}
	\min_{p} \quad & q^{(k)}(p) = f(x_k) + g^T_kp + \frac{1}{2}p^TB_kp \\
	\text{s.a.} \quad & \Vert p \Vert \leq \Delta_k. \\
\end{aligned}
\end{equation}
Notamos que en el modelo se escribe $B_k$ en lugar de $G_k$, pues no siempre se usa esta última.
Debido al coste computacional, como vimos en la elección de la dirección de búsqueda, a veces se prefiere aproximar de alguna manera más o menos eficiente, e incluso puede ser aceptable tomar la matriz $0$.

También se puede elegir qué norma define la región de confianza, cambiando así la forma de esta y ofreciendo distintos resultados, aunque generalmente se utiliza la bola definida por $\Vert p \Vert_2 \leq \Delta_k$.

La efectividad de cada iteración depende de la elección del radio $\Delta_k$, es por ello que puede que la primera elección de este no sea la definitiva. Es decir, se toma un radio a raíz de la información que se tenga, esta puede incluír la de pasos anteriores, y luego se decide si este radio nos da un resultado aceptable. Un radio demasiado pequeño nos puede hacer perder la oportunidad de ser mucho más rápidos, pero un paso demasiado grande, el mínimo de la función modelo $m_k$ puede estar lejos del mínimo de la función objetivo. Este último caso es el que se comprueba y se decide si reducir la región de confianza.

Una vez tomado el radio, encontrar el mínimo es directo en el caso de que $B_k$ sea definida positiva, basta tomar $p_k^B = -B_k^{-1}g_k$. En caso contrario tampoco supone una tarea muy costosa ya que sólo se necesita una solución aproximada para garantizar la convergencia.


\chapter{Mínimos Cuadrados}
%campos en los que se utiliza??? 

El problema de mínimos cuadrados surge de la necesidad de ajustar modelos que nos permitan predecir ciertos comportamientos en una amplia variedad de campos. Dados unos datos $(t_1,y_1),(t_2,y_2),\cdots,(t_m,y_m)$, queremos ajustar una función
$\phi(t,x)$ de forma que se minimicen los residuos $r_i(x) = \phi(t_i,x) - y_i$ para $i=1,\cdots,m$
\begin{equation}
	\min_{x\in \mathbb{R}^{n}}f(x) = \frac{1}{2} r(x)^Tr(x) = \frac{1}{2}\sum_{i=1}^{m}r_i^2(x), \quad m\geq n,
	\label{eq:lsp}
\end{equation}
donde $r(x) = (r_1(x), r_2(x), \ldots, r_m(x))^T$, con $r_i : \mathbb{R}^{n} \rightarrow \mathbb{R}$ funciones continuamente diferenciables. 

%Se asume que $m \geq n$ para este problema, de hecho en la práctica lo más común es que $m \gg n$. 

Veamos las propiedades de este modelo concreto de optimización sin restricciones y cómo se pueden aprovechar para formular algoritmos eficientes y robustos.
Sea $J(x)$ la matriz Jacobiana de $r(x)$, 
\begin{equation}
	J(x) = 
	\begin{bmatrix}
		\nabla r_1(x)^T \\
		\nabla r_2(x)^T \\
		\vdots \\
		\nabla r_m(x)^T
	\end{bmatrix}
	=
	\begin{bmatrix}
		\frac{\partial r_1}{\partial x_1}(x) &
		\frac{\partial r_1}{\partial x_2}(x) &
		\cdots &
		\frac{\partial r_1}{\partial x_n}(x) \\
		
		\frac{\partial r_2}{\partial x_1}(x) &
		\frac{\partial r_2}{\partial x_2}(x) &
		\cdots &
		\frac{\partial r_2}{\partial x_n}(x) \\
		
		\cdots &
		\cdots &
		\cdots &
		\cdots & \\
		
		\frac{\partial r_m}{\partial x_1}(x) &
		\frac{\partial r_m}{\partial x_2}(x) &
		\cdots &
		\frac{\partial r_m}{\partial x_n}(x) 
	\end{bmatrix}.
\end{equation}
El gradiente y la Hessiana de $f$ se pueden expresar como sigue:
\begin{align}
	g(x) = \nabla f(x) &= \sum_{i=1}^m r_i(x) \nabla r_i(x) = J(x)^Tr(x) \label{eq:grad}\\
	G(x) = \nabla^2 f(x) &= \sum_{i=1}^m \nabla r_i(x) \nabla r_i(x)^T + \sum_{i=1}^m r_i(x)\nabla^2r_i(x) \nonumber \\
	&= J(x)^TJ(x)+S(x). \label{eq:hess}
\end{align}

Si nos fijamos en la formulación de la matriz Hessiana, el cálculo del primer termino es directo gracias a que ya obtenemos $J(x)$ para calcular el gradiente (\ref{eq:grad}), así que el coste se reduce al segundo término, que hemos denotado $S(x)$.
Teniendo en cuenta el siguiente modelo cuadrático
%Veamos la expresión del modelo cuadrático de $f(x)$ utilizando (\ref{eq:lsp}), (\ref{eq:grad}) y (\ref{eq:hess}):
\begin{equation}
	q^{(k)}(x) = f(x_k) + g^T_k(x-x_k) + \frac{1}{2}(x-x_k)^TG_k(x-x_k),
\end{equation}
y sustituyendo según (\ref{eq:lsp}), (\ref{eq:grad}) y (\ref{eq:hess}), obtenemos el método de Newton para el problema inicial,
\begin{equation}
	x_{k+1} = x_k-(J(x_k)^TJ(x_k)+S(x_k))^{-1}J(x_k)^Tr(x_k).
\end{equation}

\section{Problema Lineal}

El primer caso más sencillo es si $\phi (t, x)$ es una función lineal, en cuyo caso los residuos
$r_i(x)$ también serán lineales.
Por ser $\phi$ lineal, se puede representar como $Jx$, con $J$ una matriz
$m\times n$.
Realizaremos un estudio del caso lineal para tener un conocimiento de como se enfocan estos problemas, que nos servirá para entender mejor el caso no lineal.
Si escribimos el vector residuo como $r(x) = Jx-y$, la función objetivo nos queda de la forma
\begin{equation}
	f(x) = \frac{1}{2} \Vert Jx-y \Vert^2.
	\label{eq:flin}
\end{equation}
En consecuencia, tomando como referencia (\ref{eq:grad}) y (\ref{eq:hess}) y teniendo en cuenta que en este caso particular $\nabla^2r_i=0$, nos queda
\begin{equation}
	\nabla f(x) = J^T(Jx-y), \qquad \nabla^2 f(x) =  J^TJ.
\end{equation}

Como $f$ es convexa, dado un punto $x^*$ tal que $\nabla f(x^*) = 0$, este será mínimo global (\ref{th:convx}). Por tanto, $x^*$ satisface el siguiente sistema lineal:
\begin{equation}
	J^TJx^* = J^Ty.
\end{equation}

Veamos cómo enfocar este sistema de ecuaciones, conocidas como \textit{ecuaciones normales} de (\ref{eq:flin}).
Lo más común en este caso para resolver numéricamente es usar distintos tipos de factorización sobre la matriz $J^TJ$ o sobre $J$, para luego resolver con sustituciones triangulares.
El primer algoritmo que se plantea es a partir de la \textbf{factorización de Cholesky}, comenzando por computar la matriz de coeficientes $J^TJ$ y el lado derecho $J^Ty$. Después se computa la factorización de Cholesky
\begin{equation}
	J^TJ = \bar R^T\bar R.
\end{equation}
Para que esta exista, necesitamos que $m \geq n$ y que $J$ sea de rango $n$,
lo que permite que $J^TJ$ sea simétrica y definida positiva.
Se termina realizando las dos sustituciones triangulares con los factores de Cholesky para encontrar $x^*$.
La principal desventaja de este método es que el condicionamiento de $J^TJ$ es el cuadrado del condicionamiento de $J$, y esto puede llevar a errores de aproximación.
Además, si $J$ está mal condicionada, ni quiera se puede llevar a cabo la factorización.

Una segunda posibilidad es basarse en la \textbf{factorización QR}, que evita el problema de depender del cuadrado del condicionamiento de $J$, ya que aplicaremos la factorización directamente a $J$. Se aprovecha que la norma Euclídea no se ve afectada por trasformaciones ortogonales para partir de la igualdad
\begin{equation}
	\Vert Jx- y \Vert = \Vert Q^T(Jx-y) \Vert, 
\end{equation}
siendo $Q$ una matriz ortogonal $m \times m$. Factorizando con una matriz pivote $\Pi$, la solución es
\begin{equation}
	x^* = \Pi R^{-1}Q_1^Ty.
\end{equation}
Donde $R$ es una matriz $n \times n$ triangular superior con elementos positivos en la diagonal y $Q_1$ son las primeras $n$ columnas de $Q$, ambos producto de la factorización QR.

En este caso, el error relativo es proporcional al condicionamiento de $J$ y no de su cuadrado. Aún así, hay situaciones en las que necesitamos asegurar que la obtención sea de algún modo más robusta o en las que queremos más información acerca de la sensibilidad de la solución a perturbaciones en $J$ o en $y$. Esto es, queremos asegurarnos que pequeños cambios en $J$ o $y$ no afecten significativamente a la solución obtenida, lo cual pondría en duda nuestro método ya que estas perturbaciones se pueden dar durante la computación.

Nos basaremos ahora en la \textbf{descomposición de valores singulares (DVS)}, cuya resolución una vez realizada la descomposición se enfoca de forma similar a la anterior. Primero se realiza el algoritmo (DVS) para obtener $J = USV^T$, con $U$ matriz $m \times m$, $V$ matriz $n \times n$, ambas ortogonales y $S$ matriz $n \times n$ de elementos diagonales $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n > 0$. Aprovechamos estas propiedades para obtener
\begin{equation}
	x^* = VS^{-1}U_1^Ty.
\end{equation}
Denotando por $u_i \in \mathbb{R}^m$ y $v_i \in \mathbb{R}^n$ las columnas de $U$ y $V$ respectivamente, escribimos
\begin{equation}
	x^* = \sum_{i=1}^n \frac{i_i^Ty}{\sigma_i}v_i.
\end{equation}
Fórmula de donde obtenemos información útil como la sensibilidad al aproximar $x^*$.

Las 3 opciones son buenas según las condiciones en las que nos encontremos. La resolución basada en Cholesky es útil cuando $m\gg n$ y es práctico trabajar almacenando $J^TJ$ en lugar de $J$, siempre y cuando $J$ sea de rango completo y bien condicionada. Si esto último no se cumple, la factorización QR es un enfoque más equilibrado, mientras que DVS es el más costoso a cambio de ser el más fiable.

Por último, mencionar que existen métodos para problemas muy grandes, en los que se usan técnicas iterativas como el método de gradientes conjugados para resolver el sistema.

%%%%%%%%     Póñense a bibliografía, Apéndices e o índice alfabético     %%%%%%%%
\appendix
\renewcommand{\thechapter}{\Roman{chapter}}
\chapter{Título del Anexo I}


\chapter{Título del Anexo II}

\backmatter

\begin{thebibliography}{99}

%%%%%%%%----Exemplo de entradas bibliográficas:
%


\bibitem{Noc} Nocedal, J., \& Wright, S. (2006). \emph{Numerical Optimization} (2nd ed.). Springer.

\bibitem{Sun} Sun, W., \& Yuan, Y.-X. (2006). \emph{Optimization theory and methods: Nonlinear programming} (2006th ed.). Springer.

%
\end{thebibliography}




\end{document}


